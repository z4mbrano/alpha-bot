"""
AlphaBot API Routes
Endpoints para upload e an√°lise de planilhas anexadas

Corre√ß√µes cr√≠ticas:
- Isolamento de sess√£o por usu√°rio (user_id + session_id)
- Processamento unificado de dados (for√ßa tipagem Quantidade/Receita/Data)
- Persist√™ncia completa de hist√≥rico de chat (user/assistant)
"""

import io
import uuid
from typing import Dict, Any
from flask import Blueprint, request, jsonify
import pandas as pd

from src.services import get_ai_service, get_data_analyzer
from src.utils import allowed_file, ALLOWED_EXTENSIONS
from src.utils.data_processor import process_dataframe_unified
import database


# Criar blueprint
alphabot_bp = Blueprint('alphabot', __name__, url_prefix='/api/alphabot')

# Armazenamento global para sess√µes do AlphaBot (ISOLADO POR USU√ÅRIO)
# Chave composta: f"{user_id}_{session_id}" quando user_id existir; caso contr√°rio, usa apenas session_id
ALPHABOT_SESSIONS: Dict[str, Dict[str, Any]] = {}


@alphabot_bp.route('/upload', methods=['POST'])
def upload():
    """
    Upload e processamento de m√∫ltiplos arquivos para o AlphaBot.
    
    Aceita arquivos .csv e .xlsx, consolida em um √∫nico DataFrame,
    cria colunas auxiliares temporais e armazena em sess√£o.
    
    Returns:
        JSON com session_id e metadados dos arquivos processados
    """
    try:
        # Verificar se h√° arquivos na requisi√ß√£o
        if 'files' not in request.files:
            return jsonify({
                "status": "error",
                "message": "Nenhum arquivo foi enviado."
            }), 400
        
        files = request.files.getlist('files')
        # user_id pode ser enviado como campo de formul√°rio junto aos arquivos
        user_id_raw = request.form.get('user_id')
        user_id = int(user_id_raw) if user_id_raw and user_id_raw.isdigit() else None
        
        if not files or len(files) == 0:
            return jsonify({
                "status": "error",
                "message": "Lista de arquivos vazia."
            }), 400
        
        # Preparar dados para o analyzer
        files_data = []
        files_failed = []
        
        for file in files:
            if not file or file.filename == '':
                continue
            
            filename = file.filename
            
            # Validar extens√£o
            if not allowed_file(filename, ALLOWED_EXTENSIONS):
                files_failed.append({
                    "filename": filename,
                    "reason": "Formato n√£o suportado (apenas .csv e .xlsx)"
                })
                continue
            
            try:
                # Ler bytes do arquivo
                file_bytes = file.read()
                files_data.append((file_bytes, filename))
            except Exception as e:
                files_failed.append({
                    "filename": filename,
                    "reason": f"Erro ao ler arquivo: {str(e)}"
                })
        
        # Usar DataAnalyzer para processar arquivos
        analyzer = get_data_analyzer()
        result = analyzer.load_files_from_bytes(files_data)
        
        # Adicionar falhas de valida√ß√£o inicial
        result['files_failed'].extend(files_failed)
        
        # Verificar se pelo menos um arquivo foi lido com sucesso
        if not result['files_ok']:
            return jsonify({
                "status": "error",
                "message": "Nenhum arquivo v√°lido foi processado.",
                "files_success": result['files_ok'],
                "files_failed": result['files_failed']
            }), 400
        
        # Consolidar tabelas
        consolidated_df = analyzer.consolidate_tables()

        # Remover duplicatas
        initial_count = len(consolidated_df)
        consolidated_df = consolidated_df.drop_duplicates()
        duplicates_removed = initial_count - len(consolidated_df)
        
        if duplicates_removed > 0:
            print(f"[AlphaBot] ‚ö†Ô∏è Removidas {duplicates_removed} linhas duplicadas")
        
        # Gerar ID de sess√£o √∫nico
        session_id = str(uuid.uuid4())
        
        # Construir sum√°rio b√°sico (antes da unifica√ß√£o) para levantar poss√≠veis colunas temporais
        summary = analyzer.build_summary(result['files_ok'], result['files_failed'])

        # üîß Aplicar processamento UNIFICADO (corrige Quantidade/Receita/Data)
        consolidated_df, processing_metadata = process_dataframe_unified(
            consolidated_df,
            source_info="AlphaBot_Upload"
        )
        
        # Construir chave composta (isolamento por usu√°rio)
        session_key = f"{user_id}_{session_id}" if user_id else session_id

        # Armazenar DataFrame em sess√£o (formato JSON) isolado por usu√°rio
        ALPHABOT_SESSIONS[session_key] = {
            "dataframe": consolidated_df.to_json(orient='split', date_format='iso'),
            "metadata": {
                "total_records": len(consolidated_df),
                "total_columns": len(consolidated_df.columns),
                "columns": list(consolidated_df.columns),
                "date_columns": [c for c, info in processing_metadata.get('columns_processed', {}).items() if info.get('type') == 'temporal'],
                "files_success": result['files_ok'],
                "files_failed": result['files_failed']
            }
        }

        # Persistir sess√£o no banco se user_id fornecido
        if user_id is not None:
            try:
                database.create_alphabot_session(
                    user_id=int(user_id),
                    session_id=session_id,
                    dataframe_json=consolidated_df.to_json(orient='split', date_format='iso'),
                    metadata={
                        "total_records": len(consolidated_df),
                        "total_columns": len(consolidated_df.columns),
                        "columns": list(consolidated_df.columns),
                        "date_columns": [c for c, info in processing_metadata.get('columns_processed', {}).items() if info.get('type') == 'temporal']
                    },
                    files_info=result['files_ok']
                )
                print(f"[AlphaBot Upload] ‚úÖ Sess√£o persistida no banco para user_id={user_id}")
            except Exception as e:
                print(f"[AlphaBot Upload] ‚ö†Ô∏è Falha ao persistir sess√£o: {e}")
        
        # Calcular per√≠odo se houver colunas de data (ap√≥s unifica√ß√£o)
        date_range = None
        date_cols = [c for c, info in processing_metadata.get('columns_processed', {}).items() if info.get('type') == 'temporal']
        if date_cols:
            try:
                min_date = None
                max_date = None
                for dc in date_cols:
                    valid = consolidated_df[dc].dropna()
                    if not valid.empty:
                        dmin = valid.min()
                        dmax = valid.max()
                        min_date = dmin if min_date is None or dmin < min_date else min_date
                        max_date = dmax if max_date is None or dmax > max_date else max_date
                if min_date or max_date:
                    date_range = {
                        "min": analyzer.format_date(min_date),
                        "max": analyzer.format_date(max_date)
                    }
            except Exception as _:
                pass
        
        # Retornar resposta de sucesso
        return jsonify({
            "status": "success",
            "message": f"{len(result['files_ok'])} arquivo(s) processado(s) com sucesso.",
            "session_id": session_id,
            "metadata": {
                "total_records": len(consolidated_df),
                "total_columns": len(consolidated_df.columns),
                "columns": list(consolidated_df.columns),
                "date_columns": date_cols,
                "files_success": result['files_ok'],
                "files_failed": result['files_failed'],
                "date_range": date_range,
                "duplicates_removed": duplicates_removed
            }
        }), 200
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"Erro interno ao processar arquivos: {str(e)}"
        }), 500


@alphabot_bp.route('/chat', methods=['POST'])
def chat():
    """
    Endpoint de chat para AlphaBot com motor de valida√ß√£o interna.
    Usa as tr√™s personas: Analista ‚Üí Cr√≠tico ‚Üí J√∫ri
    
    Returns:
        JSON com resposta do bot e metadados
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "JSON inv√°lido"}), 400
        
        session_id = data.get('session_id')
        message = data.get('message')
        conversation_id = data.get('conversation_id')
        user_id = data.get('user_id')
        
        if not session_id or not message:
            return jsonify({"error": "session_id e message s√£o obrigat√≥rios"}), 400

        # Construir chave de sess√£o isolada
        session_key = f"{user_id}_{session_id}" if user_id else session_id

        # Recuperar dados da sess√£o: preferir persist√™ncia no banco quando user_id dispon√≠vel
        df = None
        metadata = None
        if user_id:
            session_row = database.get_alphabot_session(int(user_id), session_id)
            if session_row:
                try:
                    df = pd.read_json(io.StringIO(session_row["dataframe"]), orient='split')
                    metadata = session_row["metadata"]
                except Exception:
                    df = None
                    metadata = None

        if df is None:
            # Fallback: sess√£o em mem√≥ria
            if session_key not in ALPHABOT_SESSIONS:
                return jsonify({
                    "error": "Sess√£o n√£o encontrada. Por favor, fa√ßa upload dos arquivos primeiro.",
                    "session_id": session_id
                }), 404
            session_data = ALPHABOT_SESSIONS[session_key]
            df = pd.read_json(io.StringIO(session_data["dataframe"]), orient='split')
            metadata = session_data["metadata"]
        
        # Preparar contexto dos dados para o LLM
        data_context = f"""
**Dados Dispon√≠veis:**
- Total de Registros: {metadata['total_records']}
- Total de Colunas: {metadata['total_columns']}
- Colunas: {', '.join(metadata['columns'])}
- Arquivos: {', '.join(metadata['files_success'])}
"""
        
        if metadata['date_columns']:
            data_context += f"\n- Colunas Temporais: {', '.join(metadata['date_columns'])}"
        
        # An√°lise estat√≠stica b√°sica para contexto
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            data_context += f"\n- Colunas Num√©ricas: {', '.join(numeric_cols[:5])}..."
        
        # Preparar preview dos dados (primeiras 5 linhas)
        data_preview = df.head(5).to_markdown(index=False)
        
        # Persistir mensagem do usu√°rio no hist√≥rico, garantindo conversa
        if conversation_id and user_id:
            try:
                existing = database.get_alphabot_conversation(conversation_id)
                if not existing:
                    database.create_alphabot_conversation(
                        conversation_id=conversation_id,
                        session_id=session_id,
                        user_id=int(user_id),
                        title=f"Chat AlphaBot - {pd.Timestamp.now().strftime('%d/%m/%Y %H:%M')}"
                    )
                database.add_alphabot_message(
                    conversation_id=conversation_id,
                    author='user',
                    text=message,
                    time=int(pd.Timestamp.now().timestamp() * 1000)
                )
            except Exception as e:
                print(f"[AlphaBot Chat] ‚ö†Ô∏è Falha ao salvar mensagem de usu√°rio: {e}")

        # Criar servi√ßo de IA
        ai_service = get_ai_service('alphabot')
        
        # PROMPT do motor de valida√ß√£o (Analista ‚Üí Cr√≠tico ‚Üí J√∫ri)
        validation_prompt = f"""
{data_context}

**Preview dos Dados (5 primeiras linhas):**
```
{data_preview}
```

**Pergunta do Usu√°rio:** {message}

**INSTRU√á√ïES INTERNAS (N√ÉO MOSTRE ISSO AO USU√ÅRIO):**

Simule internamente o processo de delibera√ß√£o:

1. **ANALISTA** - Execute a an√°lise t√©cnica nos dados:
   - Identifique quais colunas usar
   - Execute filtros, agrega√ß√µes, rankings necess√°rios
   - Formule uma resposta preliminar baseada nos dados

2. **CR√çTICO** - Desafie a an√°lise:
   - H√° vieses ou suposi√ß√µes n√£o validadas?
   - Faltam dados importantes para esta an√°lise?
   - H√° interpreta√ß√µes alternativas?
   
3. **J√öRI** - Sintetize a resposta final no formato:
   - **Resposta Direta:** [Uma frase clara]
   - **An√°lise Detalhada:** [Como chegou ao resultado]
   - **Insights Adicionais:** [Observa√ß√µes valiosas]
   - **Limita√ß√µes e Contexto:** [Se aplic√°vel]

Apresente APENAS a resposta final do J√∫ri ao usu√°rio.
"""
        
        # Gerar resposta
        answer = ai_service.generate_response(validation_prompt)

        # Persistir resposta do bot
        if conversation_id and user_id:
            try:
                # ATEN√á√ÉO: schema aceita apenas 'user' ou 'bot'
                database.add_alphabot_message(
                    conversation_id=conversation_id,
                    author='bot',
                    text=answer,
                    time=int(pd.Timestamp.now().timestamp() * 1000)
                )
            except Exception as e:
                print(f"[AlphaBot Chat] ‚ö†Ô∏è Falha ao salvar resposta do bot: {e}")
        
        return jsonify({
            "answer": answer,
            "session_id": session_id,
            "metadata": {
                "records_analyzed": len(df),
                "columns_available": len(df.columns)
            }
        }), 200
        
    except Exception as e:
        return jsonify({
            "error": f"Erro ao processar pergunta: {str(e)}",
            "session_id": session_id if 'session_id' in locals() else None
        }), 500


@alphabot_bp.route('/session/<session_id>', methods=['GET'])
def get_session(session_id: str):
    """
    Obt√©m informa√ß√µes sobre uma sess√£o existente.
    
    Args:
        session_id: ID da sess√£o
    
    Returns:
        JSON com metadados da sess√£o
    """
    if session_id not in ALPHABOT_SESSIONS:
        return jsonify({
            "error": "Sess√£o n√£o encontrada",
            "session_id": session_id
        }), 404
    
    metadata = ALPHABOT_SESSIONS[session_id]['metadata']
    
    return jsonify({
        "session_id": session_id,
        "metadata": metadata
    }), 200


@alphabot_bp.route('/session/<session_id>', methods=['DELETE'])
def delete_session(session_id: str):
    """
    Remove uma sess√£o existente.
    
    Args:
        session_id: ID da sess√£o
    
    Returns:
        JSON confirmando a remo√ß√£o
    """
    if session_id not in ALPHABOT_SESSIONS:
        return jsonify({
            "error": "Sess√£o n√£o encontrada",
            "session_id": session_id
        }), 404
    
    del ALPHABOT_SESSIONS[session_id]
    
    return jsonify({
        "message": "Sess√£o removida com sucesso",
        "session_id": session_id
    }), 200
