"""
Data Analyzer Service
Gerencia an√°lise e sumariza√ß√£o de dados
"""

from typing import Any, Dict, List, Optional, Tuple
import pandas as pd
import numpy as np

from ..utils.data_processors import prepare_table
from ..utils.file_handlers import load_csv_tables, load_excel_tables, load_from_bytes


class DataAnalyzer:
    """Servi√ßo para an√°lise e consolida√ß√£o de dados de m√∫ltiplas fontes."""
    
    def __init__(self):
        """Inicializa o analisador de dados."""
        self.tables: List[Dict[str, Any]] = []
        self.consolidated_df: Optional[pd.DataFrame] = None
        self.summary: Optional[Dict[str, Any]] = None
    
    def load_files_from_drive(
        self,
        drive_service: Any,
        file_metas: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Carrega e processa arquivos do Google Drive.
        
        Args:
            drive_service: Inst√¢ncia do servi√ßo do Google Drive
            file_metas: Lista de metadados dos arquivos
        
        Returns:
            Dict com resumo do carregamento (files_ok, files_failed, tables)
        """
        files_ok: List[str] = []
        files_failed: List[Dict[str, str]] = []
        
        for file_meta in file_metas:
            try:
                mime_type = file_meta.get('mimeType', '')
                
                if 'spreadsheet' in mime_type or mime_type == 'text/csv':
                    if mime_type == 'text/csv':
                        tables = load_csv_tables(drive_service, file_meta)
                    else:
                        tables = load_excel_tables(drive_service, file_meta)
                    
                    self.tables.extend(tables)
                    files_ok.append(file_meta['name'])
                else:
                    files_failed.append({
                        'name': file_meta['name'],
                        'reason': f"Formato n√£o suportado: {mime_type}"
                    })
            
            except Exception as error:
                files_failed.append({
                    'name': file_meta['name'],
                    'reason': str(error)
                })
        
        return {
            'files_ok': files_ok,
            'files_failed': files_failed,
            'tables': self.tables
        }
    
    def load_files_from_bytes(
        self,
        files_data: List[Tuple[bytes, str]]
    ) -> Dict[str, Any]:
        """
        Carrega arquivos a partir de bytes (para upload direto).
        
        Args:
            files_data: Lista de tuplas (file_bytes, filename)
        
        Returns:
            Dict com resumo do carregamento
        """
        files_ok: List[str] = []
        files_failed: List[Dict[str, str]] = []
        
        for file_bytes, filename in files_data:
            try:
                tables = load_from_bytes(file_bytes, filename)
                self.tables.extend(tables)
                files_ok.append(filename)
            
            except Exception as error:
                files_failed.append({
                    'name': filename,
                    'reason': str(error)
                })
        
        return {
            'files_ok': files_ok,
            'files_failed': files_failed,
            'tables': self.tables
        }
    
    def consolidate_tables(self) -> pd.DataFrame:
        """
        Consolida todas as tabelas carregadas em um √∫nico DataFrame.
        
        Returns:
            DataFrame consolidado
        
        Raises:
            ValueError: Se n√£o houver tabelas carregadas
        """
        if not self.tables:
            raise ValueError("Nenhuma tabela carregada para consolidar")
        
        dfs = [table['df'] for table in self.tables]
        self.consolidated_df = pd.concat(dfs, ignore_index=True)
        
        return self.consolidated_df
    
    def build_summary(
        self,
        files_ok: List[str],
        files_failed: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """
        Constr√≥i sum√°rio dos dados carregados.
        
        Args:
            files_ok: Lista de arquivos carregados com sucesso
            files_failed: Lista de arquivos que falharam
        
        Returns:
            Dict com sum√°rio completo
        """
        total_records = sum(table['row_count'] for table in self.tables)
        all_columns = set()
        numeric_columns = set()
        text_columns = set()
        datetime_columns_names = set()
        start_dates: List[pd.Timestamp] = []
        end_dates: List[pd.Timestamp] = []

        for table in self.tables:
            all_columns.update(table['columns'])
            numeric_columns.update(table['numeric_columns'])
            text_columns.update(table['text_columns'])
            datetime_columns_names.update(table['datetime_columns'].keys())

            for parsed in table['datetime_columns'].values():
                valid = parsed.dropna()
                if not valid.empty:
                    start_dates.append(valid.min())
                    end_dates.append(valid.max())

        if start_dates and end_dates:
            date_range: Tuple[Optional[pd.Timestamp], Optional[pd.Timestamp]] = (
                min(start_dates),
                max(end_dates),
            )
        else:
            date_range = (None, None)

        domains: List[str] = []
        if numeric_columns:
            domains.append('num√©rico')
        if text_columns:
            domains.append('categ√≥rico')
        if datetime_columns_names:
            domains.append('temporal')

        self.summary = {
            'files_ok': files_ok,
            'files_failed': files_failed,
            'total_records': int(total_records),
            'columns': sorted(filter(None, all_columns)),
            'numeric_columns': sorted(numeric_columns),
            'text_columns': sorted(text_columns),
            'datetime_columns': sorted(datetime_columns_names),
            'date_range': date_range,
            'domains': domains,
        }
        
        return self.summary
    
    @staticmethod
    def format_date(date_value: Optional[pd.Timestamp]) -> Optional[str]:
        """
        Formata um timestamp para string DD/MM/YYYY.
        
        Args:
            date_value: Timestamp do pandas
        
        Returns:
            String formatada ou None
        """
        if date_value is None or (isinstance(date_value, float) and np.isnan(date_value)):
            return None
        try:
            timestamp = pd.to_datetime(date_value)
        except Exception:
            return None
        if pd.isna(timestamp):
            return None
        return timestamp.strftime('%d/%m/%Y')
    
    def build_discovery_report(self, summary: Optional[Dict[str, Any]] = None) -> str:
        """
        Constr√≥i relat√≥rio markdown de descoberta dos dados.
        
        Args:
            summary: Sum√°rio dos dados (usa self.summary se n√£o fornecido)
        
        Returns:
            String markdown com o relat√≥rio formatado
        """
        if summary is None:
            summary = self.summary
        
        if not summary:
            raise ValueError("Sum√°rio n√£o dispon√≠vel. Execute build_summary() primeiro.")
        
        files_ok = summary['files_ok']
        files_failed = summary['files_failed']
        date_range = summary['date_range']

        files_ok_md = '\n'.join(f"- {name}" for name in files_ok) or '- Nenhum arquivo processado.'
        files_failed_md = '\n'.join(
            f"- {entry['name']} (Motivo: {entry['reason']})" for entry in files_failed
        ) or '- Nenhum arquivo com falha.'

        start_text = self.format_date(date_range[0])
        end_text = self.format_date(date_range[1])
        if start_text and end_text:
            period_text = f"{start_text} at√© {end_text}"
        elif start_text or end_text:
            period_text = start_text or end_text
        else:
            period_text = 'N√£o identificado'

        numeric_cols_md = ', '.join(f"`{col}`" for col in summary['numeric_columns']) or 'Nenhum'
        text_cols_md = ', '.join(f"`{col}`" for col in summary['text_columns']) or 'Nenhum'
        
        # Diagn√≥stico de colunas temporais
        datetime_cols = summary.get('datetime_columns', [])
        if datetime_cols:
            datetime_cols_md = ', '.join(f"`{col}`" for col in datetime_cols)
            datetime_status = f"""#### üìÖ Campos Temporais (An√°lises de Evolu√ß√£o)
**Status da Convers√£o de Datas:**
- **‚úÖ Convers√£o Bem-Sucedida:** {datetime_cols_md}
  - **Capacidades:** Filtros por ano, m√™s, trimestre, per√≠odo, evolu√ß√£o temporal
"""
        else:
            datetime_status = """#### üìÖ Campos Temporais
**Status:** Nenhuma coluna temporal detectada
- An√°lises de evolu√ß√£o temporal n√£o est√£o dispon√≠veis neste dataset
"""

        can_temporal = "‚úÖ" if datetime_cols else "‚ùå"

        return f"""## üîç Descoberta e Diagn√≥stico Completo

**Status:** Leitura, processamento e diagn√≥stico finalizados ‚úÖ

### üìÅ Arquivos Processados com Sucesso
{files_ok_md}

### ‚ö†Ô∏è Arquivos Ignorados/Com Falha
{files_failed_md}

---

### üó∫Ô∏è MAPA DO ECOSSISTEMA DE DADOS

**Registros Totais Consolidados:** {summary['total_records']}

**Per√≠odo Identificado:** {period_text}

---

### üî¨ DIAGN√ìSTICO DE QUALIDADE POR TIPO

#### üí∞ Campos Num√©ricos (An√°lises Quantitativas)
**Prontos para:** soma, m√©dia, m√≠nimo, m√°ximo, contagem

{numeric_cols_md}

#### üìù Campos Categ√≥ricos (Agrupamentos e Filtros)
**Prontos para:** agrupamento, ranking, filtros

{text_cols_md}

{datetime_status}

---

### üéØ CAPACIDADES ANAL√çTICAS DISPON√çVEIS

Com base no diagn√≥stico acima, **posso responder perguntas sobre:**

‚úÖ **Totaliza√ß√µes:** Soma, m√©dia, contagem nos campos num√©ricos
‚úÖ **Rankings:** Top N por qualquer campo categ√≥rico
‚úÖ **Filtros:** Por categorias dispon√≠veis
{can_temporal} **An√°lises Temporais:** Evolu√ß√£o, compara√ß√£o de per√≠odos
‚úÖ **Compara√ß√µes:** Entre categorias
‚úÖ **Detalhamento:** Drill-down em registros espec√≠ficos

---

**Status:** Ecossistema mapeado. Pronto para an√°lises investigativas. üöÄ
"""
    
    def get_column_info(self, column_name: str) -> Optional[Dict[str, Any]]:
        """
        Obt√©m informa√ß√µes sobre uma coluna espec√≠fica.
        
        Args:
            column_name: Nome da coluna
        
        Returns:
            Dict com info da coluna ou None se n√£o encontrada
        """
        if not self.consolidated_df is not None:
            return None
        
        if column_name not in self.consolidated_df.columns:
            return None
        
        series = self.consolidated_df[column_name]
        
        info = {
            'name': column_name,
            'dtype': str(series.dtype),
            'count': int(series.count()),
            'null_count': int(series.isnull().sum()),
            'unique_count': int(series.nunique()),
        }
        
        if pd.api.types.is_numeric_dtype(series):
            info.update({
                'min': float(series.min()) if not series.empty else None,
                'max': float(series.max()) if not series.empty else None,
                'mean': float(series.mean()) if not series.empty else None,
                'median': float(series.median()) if not series.empty else None,
            })
        
        return info


def get_data_analyzer() -> DataAnalyzer:
    """
    Factory function para obter inst√¢ncia do DataAnalyzer.
    
    Returns:
        Inst√¢ncia do DataAnalyzer
    """
    return DataAnalyzer()
