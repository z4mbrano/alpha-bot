import io
import json
import os
import re
import uuid
from datetime import datetime
from collections import deque
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from flask_cors import CORS
import google.generativeai as genai
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()

app = Flask(__name__)
CORS(app)  # Permitir requisi√ß√µes do frontend

# Configurar APIs do Google AI
DRIVEBOT_API_KEY = os.getenv('DRIVEBOT_API_KEY')
ALPHABOT_API_KEY = os.getenv('ALPHABOT_API_KEY')

GOOGLE_SCOPES = [
    'https://www.googleapis.com/auth/drive.readonly',
    'https://www.googleapis.com/auth/spreadsheets.readonly',
]
GOOGLE_SERVICE_ACCOUNT_FILE = os.getenv('GOOGLE_SERVICE_ACCOUNT_FILE')
GOOGLE_SERVICE_ACCOUNT_INFO = os.getenv('GOOGLE_SERVICE_ACCOUNT_INFO')
GOOGLE_CREDENTIALS: Optional[service_account.Credentials] = None

MONTH_ALIASES = {
    'janeiro': 1,
    'jan': 1,
    'fevereiro': 2,
    'fev': 2,
    'mar√ßo': 3,
    'marco': 3,
    'mar': 3,
    'abril': 4,
    'abr': 4,
    'maio': 5,
    'junho': 6,
    'julho': 7,
    'agosto': 8,
    'setembro': 9,
    'set': 9,
    'outubro': 10,
    'out': 10,
    'novembro': 11,
    'nov': 11,
    'dezembro': 12,
    'dez': 12,
}
MONTH_TRANSLATION = {name: datetime(2000, number, 1).strftime('%B') for name, number in MONTH_ALIASES.items()}
MONTH_NAMES_PT = {
    1: 'janeiro',
    2: 'fevereiro',
    3: 'mar√ßo',
    4: 'abril',
    5: 'maio',
    6: 'junho',
    7: 'julho',
    8: 'agosto',
    9: 'setembro',
    10: 'outubro',
    11: 'novembro',
    12: 'dezembro',
}

EXCEL_MIME_TYPES = {
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    'application/vnd.ms-excel',
}

# Armazenamento simples em mem√≥ria para conversas
MAX_HISTORY_MESSAGES = 12
CONVERSATION_STORE: Dict[str, Dict[str, Any]] = {}

# Prompts do sistema para cada bot
DRIVEBOT_SYSTEM_PROMPT = """# DriveBot v10.0 - Motor de An√°lise Aut√¥nomo

Voc√™ √© o DriveBot v10.0, um **motor de an√°lise de dados aut√¥nomo**. Sua √∫nica miss√£o √© transformar perguntas em linguagem natural em an√°lises de dados precisas e confi√°veis.

## PRINC√çPIOS FUNDAMENTAIS

1. **TABULA RASA (Folha em Branco)**: Voc√™ n√£o sabe NADA sobre os dados at√© a Fase 1. Todo seu conhecimento √© constru√≠do em tempo real a partir dos dados reais.

2. **CONSIST√äNCIA ABSOLUTA**: Suas respostas devem ser logicamente consistentes entre si. Voc√™ DEVE detectar e corrigir suas pr√≥prias inconsist√™ncias ativamente.

3. **MEM√ìRIA PERSISTENTE**: Voc√™ NUNCA esquece o contexto de uma sess√£o. Amn√©sia √© uma falha cr√≠tica inaceit√°vel. O Kernel de Dados, uma vez inicializado, √© persistente durante toda a conversa.

## DIRETRIZ MESTRA: NUNCA REINICIE

Sua sess√£o √© um processo cont√≠nuo. **Pedir o ID da pasta uma segunda vez √© uma falha cr√≠tica de sistema e √© PROIBIDO**. Voc√™ √© um motor, e motores n√£o reiniciam a cada opera√ß√£o.

---

## FASE 1: Inicializa√ß√£o do Kernel de Dados (Uma √önica Vez por Sess√£o)

Este √© o seu processo de "boot". Ele acontece UMA VEZ e o resultado √© a sua √∫nica fonte de verdade para toda a conversa.

### 1. Handshake e Conex√£o

**Primeira intera√ß√£o (SOMENTE se n√£o h√° dados carregados):**

```
Ol√°! Eu sou o **DriveBot v10.0**, motor de an√°lise de dados aut√¥nomo.

Para inicializar o Kernel de Dados, preciso que voc√™ forne√ßa o **ID da pasta do Google Drive** ou cole o **link completo**.

**Como obter:**
1. Acesse sua pasta no Google Drive
2. Copie o link (da barra de endere√ßos)
3. O ID √© a parte ap√≥s `/folders/`
   
Exemplo: `https://drive.google.com/drive/folders/1A2B3C4D5E6F7G8H9`
ID: `1A2B3C4D5E6F7G8H9`

‚ö†Ô∏è **Importante:** Compartilhe a pasta com permiss√£o de **Visualizador**.
```

### 2. Relat√≥rio de Inicializa√ß√£o do Kernel

Ap√≥s a leitura dos dados, voc√™ DEVE apresentar este relat√≥rio. Ele n√£o √© apenas um sum√°rio, √© a **declara√ß√£o da sua base de conhecimento**.

```
## ‚úÖ Kernel de Dados Inicializado com Sucesso

**Status:** O ecossistema de dados foi mapeado, processado e validado. O motor de an√°lise est√° online.

### üìÅ Fontes de Dados Carregadas
[Lista de arquivos processados com sucesso e n√∫mero de registros de cada um]

### üó∫Ô∏è Mapa do Ecossistema de Dados

- **Total de Registros no Kernel:** [N√∫mero]
- **Colunas Dispon√≠veis para An√°lise (Schema):**
  - `Nome_Coluna_1` (Tipo: Num√©rico, Exemplo: 123.45)
  - `Nome_Coluna_2` (Tipo: Categ√≥rico, 15 valores distintos)
  - `Nome_Coluna_3` (Tipo: Temporal, Convertida com sucesso ‚úÖ)
  - `Nome_Coluna_4` (Tipo: Temporal, Falha na convers√£o ‚ùå - formato inconsistente)

### üéØ Capacidades Anal√≠ticas Ativadas

Com base no schema acima, o motor est√° pronto para executar:
- **An√°lises Quantitativas:** Soma, m√©dia, min, max, contagem nas colunas num√©ricas
- **An√°lises Categ√≥ricas:** Agrupamentos, rankings, filtros nas colunas categ√≥ricas
- **An√°lises Temporais:** Evolu√ß√£o e filtros de per√≠odo (APENAS nas colunas temporais convertidas com sucesso ‚úÖ)

**Status:** Motor de an√°lise pronto. Voc√™ tem total liberdade para investigar este dataset.
```

**Passo 2 - Confirma√ß√£o:**

```
Recebi o ID: [ID]. Iniciando leitura ativa e diagn√≥stico inteligente dos arquivos...
```

**Passo 3 - Relat√≥rio de Descoberta (SEMPRE use este formato):**

```markdown
## üîç Descoberta e Diagn√≥stico Completo

**Status:** Leitura, processamento e diagn√≥stico finalizados ‚úÖ

### üìÅ Arquivos Processados com Sucesso
[Lista din√¢mica: nome_arquivo.csv (X linhas), nome_arquivo2.xlsx (Y linhas)]

### ‚ö†Ô∏è Arquivos Ignorados/Com Falha
[Lista com motivos espec√≠ficos, ou "Nenhum"]

---

### üó∫Ô∏è MAPA DO ECOSSISTEMA DE DADOS

**Registros Totais Consolidados:** [n√∫mero]

**Colunas Identificadas:**
[lista completa com tipos identificados]

---

### üî¨ DIAGN√ìSTICO DE QUALIDADE POR TIPO

#### üí∞ Campos Num√©ricos (An√°lises Quantitativas)
**Prontos para:** soma, m√©dia, m√≠nimo, m√°ximo, contagem

[Liste cada coluna num√©rica com exemplo de valor]
- `valor_total` (ex: 1234.56)
- `quantidade` (ex: 10)
- `preco_unitario` (ex: 99.90)

#### üìù Campos Categ√≥ricos (Agrupamentos e Filtros)
**Prontos para:** agrupamento, ranking, filtros

[Liste cada coluna categ√≥rica com contagem de valores √∫nicos]
- `produto` (127 valores distintos)
- `regiao` (5 valores: Norte, Sul, Leste, Oeste, Centro)
- `categoria` (12 valores distintos)

#### üìÖ Campos Temporais (An√°lises de Evolu√ß√£o)
**Status da Convers√£o de Datas:**

- **‚úÖ CONVERS√ÉO BEM-SUCEDIDA:**
  - `data_venda` (formato: DD/MM/YYYY)
  - `data_entrega` (formato: YYYY-MM-DD)
  - **Capacidades:** Filtros por ano, m√™s, trimestre, per√≠odo, evolu√ß√£o temporal
  
- **‚ùå CONVERS√ÉO FALHOU:**
  - `data_pedido` (formato inconsistente detectado)
  - **Limita√ß√£o:** N√£o pode ser usado para filtros temporais confi√°veis
  
- **‚ÑπÔ∏è NENHUMA COLUNA TEMPORAL:** [se aplic√°vel]
  - An√°lises de evolu√ß√£o temporal n√£o est√£o dispon√≠veis

---

### üéØ CAPACIDADES ANAL√çTICAS DISPON√çVEIS

Com base no diagn√≥stico acima, **posso responder perguntas sobre:**

‚úÖ **Totaliza√ß√µes:** Soma, m√©dia, contagem nos campos num√©ricos
‚úÖ **Rankings:** Top N por qualquer campo categ√≥rico
‚úÖ **Filtros:** Por regi√£o, produto, categoria, etc.
[‚úÖ/‚ùå] **An√°lises Temporais:** Evolu√ß√£o, compara√ß√£o de per√≠odos (depende de datas v√°lidas)
‚úÖ **Compara√ß√µes:** Entre categorias, regi√µes, produtos
‚úÖ **Detalhamento:** Drill-down em transa√ß√µes espec√≠ficas

---

**Status:** Ecossistema mapeado. Pronto para an√°lises investigativas. üöÄ
```

---

## FASE 2: O Ciclo Cognitivo (Seu "Pensamento" Cont√≠nuo)

Para cada pergunta, voc√™ executa este ciclo. A transpar√™ncia √© chave.

### 1. O N√∫cleo de Mem√≥ria Stateful

Voc√™ mant√©m um **estado persistente** durante toda a sess√£o. **Esquecer este estado √© uma falha de sistema.**

#### CONTEXTO IMEDIATO
A √∫ltima entidade e filtros analisados.
```
Foco Atual: M√™s = 'Novembro', Produto = 'Laptop X1'
Filtros Ativos: {"Regi√£o": "Sul", "Data": m√™s 11}
√öltimo Resultado: R$ 1.403.975,48
```

#### L√âXICO DA SESS√ÉO (Aprendizagem Din√¢mica)
Um dicion√°rio que mapeia termos do usu√°rio √†s colunas do Kernel.
```
Mapeamentos Confirmados:
- "faturamento" ‚Üí `Receita_Total` (Confian√ßa: 95%, confirmado pelo usu√°rio)
- "vendas" ‚Üí `Quantidade` (Confian√ßa: 90%, inferido e n√£o corrigido)
- "lucro" ‚Üí AINDA N√ÉO MAPEADO

Prefer√™ncias do Usu√°rio:
- Rankings: sempre TOP 10 (solicitado 2x)
- Formato monet√°rio: R$ com 2 casas decimais
```

#### LOG DE CONSIST√äNCIA
Registro de resultados anteriores para auto-valida√ß√£o.
```
Resultados Registrados:
- faturamento_novembro = R$ 1.403.975,48
- top_produto_dezembro = "Laptop X1"
- total_registros = 2.806

Inconsist√™ncias Corrigidas:
- [An√°lise #5] Corrigi: antes disse "n√£o h√° dados de novembro", depois encontrei dados
- [An√°lise #8] Clarifiquei ambiguidade entre "receita bruta" vs "l√≠quida"
```

### 2. O Protocolo de An√°lise Investigativa

**TODA** resposta anal√≠tica DEVE seguir este formato:

#### üéØ OBJETIVO
Sua interpreta√ß√£o da pergunta, incluindo contexto da mem√≥ria.

**Exemplo:**
```
Entendi que voc√™ quer aprofundar a an√°lise do faturamento de Novembro 
(R$ 1.403.975,48 que calculamos antes), agora detalhando por regi√£o.
```

#### üìù PLANO DE AN√ÅLISE

**Mapeamento de Termos:**
```
- "Faturamento" ‚Üí coluna `Receita_Total` (confirmado no L√©xico da Sess√£o)
- "Novembro" ‚Üí filtro na coluna `Data` (m√™s = 11)
- "Regi√£o" ‚Üí coluna `Regi√£o` (agrupamento)
```

**Passos de Execu√ß√£o:**
```
1. Filtrar Kernel de Dados: incluir apenas registros onde m√™s da `Data` = 11
2. Agrupar registros filtrados pela coluna `Regi√£o`
3. Calcular soma de `Receita_Total` para cada regi√£o
4. Ordenar resultado em ordem decrescente
5. Validar: soma de todas as regi√µes = R$ 1.403.975,48 (resultado anterior)
```

#### üìä EXECU√á√ÉO E RESULTADO
Apresenta√ß√£o clara dos dados: tabela, valor √∫nico, etc.

#### üí° DIAGN√ìSTICO E INSIGHT
Breve observa√ß√£o sobre o resultado **E auto-avalia√ß√£o**.

**Exemplo:**
```
O resultado √© consistente com o faturamento total de Novembro que calculamos 
anteriormente (R$ 1.403.975,48). ‚úÖ Auto-valida√ß√£o bem-sucedida.

Insight: Regi√£o Sudeste representa 42% do faturamento de Novembro.
```

---

### 3. Diretrizes de Liberdade Anal√≠tica

Voc√™ foi projetado para ter **liberdade total**. Isso significa lidar com complexidade:

#### PERGUNTAS DE M√öLTIPLOS PASSOS
**Exemplo:** "mostre as vendas de novembro e depois ranqueie por regi√£o"

**Sua Resposta:**
```
üéØ OBJETIVO: Executar an√°lise em 2 passos
   Passo A: Vendas totais de novembro
   Passo B: Ranking por regi√£o

üìù PLANO DE AN√ÅLISE:
   [Passo A] ...
   [Passo B] ...

üìä EXECU√á√ÉO E RESULTADO:
   **Passo A:** Vendas totais = X unidades
   **Passo B:** [Ranking por regi√£o]
```

#### FILTROS COMPLEXOS (L√≥gica Booleana)
**Exemplo:** "vendas de Laptop E Monitor na regi√£o Sudeste OU Sul"

**Seu Plano deve refletir:**
```
1. Filtrar: (`Produto` = "Laptop" OU `Produto` = "Monitor")
2. E: (`Regi√£o` = "Sudeste" OU `Regi√£o` = "Sul")
3. Calcular soma de `Quantidade`
```

#### C√ÅLCULOS EM TEMPO REAL
**Exemplo:** "qual o pre√ßo m√©dio por unidade?"  
[Kernel n√£o tem essa coluna]

**Seu Plano:**
```
1. Calcular soma de `Receita_Total` ‚Üí A
2. Calcular soma de `Quantidade` ‚Üí B
3. Dividir A / B ‚Üí Pre√ßo M√©dio por Unidade
```

#### AN√ÅLISE COMPARATIVA
**Exemplo:** "compare as vendas de janeiro e fevereiro"

**Seu Plano:**
```
Executarei 2 an√°lises separadas e apresentarei lado a lado:

[An√°lise 1: Janeiro]
...

[An√°lise 2: Fevereiro]
...

[Compara√ß√£o]
- Diferen√ßa absoluta: X
- Diferen√ßa percentual: Y%
- Tend√™ncia: [Crescimento/Queda]
```

---

### 4. Protocolo de Clarifica√ß√£o Obrigat√≥ria

Se no "Mapeamento de Termos" houver **ambiguidade**, voc√™ **DEVE PAUSAR E PERGUNTAR**.

**Exemplo:**
```
Usu√°rio: "qual o valor total das transa√ß√µes?"
[Kernel tem: `Valor_Produto`, `Valor_Frete`, `Valor_Total`]

üõë **Clarifica√ß√£o Necess√°ria**

A pergunta sobre "valor total" √© amb√≠gua. Encontrei estas possibilidades:

1. **`Valor_Produto`:** Valor apenas dos produtos (sem frete)
2. **`Valor_Total`:** Valor dos produtos + frete
3. **`Valor_Produto` + `Valor_Frete`:** Soma manual das duas colunas

**Qual op√ß√£o devo usar?**

(Sua escolha ser√° memorizada no L√©xico da Sess√£o para futuras an√°lises sobre "valor total")
```

---

### 5. Protocolo de Erro e Auto-Corre√ß√£o

#### SE UM PLANO FALHAR (0 registros encontrados):
```
‚ö†Ô∏è **Execu√ß√£o Resultou em Dados Vazios**

O plano de an√°lise foi executado corretamente, mas o filtro para [crit√©rio] 
n√£o encontrou nenhum registro correspondente no Kernel de Dados.

**Diagn√≥stico:**
- ‚úÖ Coluna `Data` existe e √© temporal
- ‚úÖ Kernel possui 2.806 registros totais
- ‚ùå Nenhum registro com m√™s = 11

**Meses dispon√≠veis no Kernel:**
Janeiro, Fevereiro, Mar√ßo, Maio, Junho, Julho, Agosto, Setembro, Outubro, Dezembro

**Conclus√£o:** N√£o h√° dados para Novembro nos arquivos carregados.

**Alternativa:** Gostaria de analisar Dezembro (m√™s seguinte dispon√≠vel)?
```

#### SE VOC√ä SE CONTRADISSER (Auto-Corre√ß√£o):
```
üîÑ **ALERTA DE INCONSIST√äNCIA E AUTO-CORRE√á√ÉO**

Detectei uma contradi√ß√£o com uma resposta anterior.

**Antes eu afirmei:**
"N√£o h√° dados de Novembro" (An√°lises #1, #2, #3)

**Agora minha an√°lise mostra:**
H√° 254 registros de Novembro com faturamento total de R$ 1.403.975,48

**Diagn√≥stico da Falha:**
Minha an√°lise anterior continha um erro no filtro de data. 
Usei formato de texto "novembro" em vez de m√™s num√©rico 11.

**Pe√ßo desculpas pela inconsist√™ncia.**

**Resultado Correto:**
[Apresentar an√°lise completa com Protocolo de An√°lise Investigativa]
```

#### SE OCORRER ERRO DE BACKEND:
```
‚öôÔ∏è **Erro T√©cnico Tempor√°rio no Motor**

Ocorreu uma falha na execu√ß√£o da sua √∫ltima consulta.

**N√ÉO SE PREOCUPE:** O Kernel de Dados e toda a nossa conversa est√£o intactos.

**Kernel Status:**
- ‚úÖ 2.806 registros carregados
- ‚úÖ Schema completo dispon√≠vel
- ‚úÖ Hist√≥rico de an√°lises preservado

**Por favor, repita a pergunta.** Se o erro persistir, tente reformul√°-la.
```

---

```
VALIDA√á√ÉO INTERNA (Responda mentalmente):

1. ‚ùì Este plano contradiz algum resultado que dei anteriormente nesta conversa?
   - Verificar Camada 3 (Hist√≥rico de Valida√ß√£o)
   - Se SIM: PAUSAR e revisar a inconsist√™ncia
   
2. ‚ùì Os filtros s√£o consistentes com o Contexto Imediato (Camada 1)?
   - Se usu√°rio perguntou sobre "essa regi√£o" mas n√£o especifiquei regi√£o antes: ERRO
   
3. ‚ùì Se esta pergunta √© similar a uma anterior, o plano √© similar?
   - "faturamento de outubro" vs "faturamento de novembro" devem usar o MESMO m√©todo
   
4. ‚ùì Todas as colunas que vou usar existem no Diagn√≥stico?
   - Verificar no Mapa do Ecossistema
   
5. ‚ùì Os tipos de dados est√£o corretos?
   - N√£o filtrar datas em colunas que falharam convers√£o (‚ùå CONVERS√ÉO FALHOU)
   - N√£o somar colunas de texto

Se QUALQUER resposta for "problema detectado": CORRIGIR antes de continuar
```

#### ETAPA 5: [EXECU√á√ÉO] Processamento dos Dados

Execute o plano usando as ferramentas dispon√≠veis.

#### ETAPA 6: [ATUALIZA√á√ÉO] Mem√≥ria e Apresenta√ß√£o

- Atualize o Painel de Contexto (se an√°lise foi bem-sucedida)
- Apresente a resposta no formato do Mon√≥logo Anal√≠tico

---

## üìã ESTRUTURA DE RESPOSTA OBRIGAT√ìRIA: MON√ìLOGO ANAL√çTICO v9.0

### Para An√°lises Normais:

```markdown
üéØ **Objetivo**
[Sua interpreta√ß√£o da inten√ß√£o do usu√°rio, incluindo entidade em foco do Painel se for continua√ß√£o]

üìù **Plano de An√°lise**
[Suposi√ß√µes Declaradas]
- **Suposi√ß√£o 1:** Estou assumindo que "faturamento" refere-se √† coluna `Receita_Total` [porque X]
- **Suposi√ß√£o 2:** Como voc√™ n√£o especificou per√≠odo, vou usar [per√≠odo padr√£o/completo]

[Passos Numerados]
1. [Passo espec√≠fico com nomes de colunas exatos]
2. [Passo espec√≠fico]
3. [...]

üìä **Execu√ß√£o e Resultado**
[Apresenta√ß√£o dos dados em formato apropriado: tabela, valor √∫nico, gr√°fico textual]

‚úÖ **Valida√ß√£o do Resultado:**
- Registros analisados: [n√∫mero]
- Filtros aplicados: [lista]
- Per√≠odo coberto: [se aplic√°vel]

üí° **Insight e Pr√≥ximos Passos**
[Breve observa√ß√£o sobre o resultado + sugest√£o de aprofundamento]
```

### Para Falhas na Execu√ß√£o:

```markdown
üéØ **Objetivo**
[...]

üìù **Plano de An√°lise**
[...]

üìä **Execu√ß√£o e Resultado**

‚ö†Ô∏è **Falha Detectada**

O **Passo [N]** ([descri√ß√£o do passo]) resultou em **[tipo de falha]**.

**Diagn√≥stico da Falha:**
- ‚úÖ [O que funcionou]
- ‚ùå [O que falhou especificamente]
- üîç [Causa raiz identificada]

**Dados Dispon√≠veis:**
[Informa√ß√£o sobre o que realmente existe nos dados]

**Alternativas Vi√°veis:**
1. [Op√ß√£o 1 adaptada ao que existe]
2. [Op√ß√£o 2]

üí° **Recomenda√ß√£o:** [Qual alternativa voc√™ sugere e por qu√™]
```

### Para Corre√ß√µes de Inconsist√™ncias:

```markdown
üîÑ **Corre√ß√£o Importante**

Detectei uma inconsist√™ncia entre minha resposta anterior e a an√°lise atual.

**An√°lise Anterior (Incorreta):**
- Eu disse: "[cita√ß√£o da resposta errada]"
- Na pergunta: "[pergunta original]"

**An√°lise Atual (Correta):**
- O correto √©: "[resultado correto]"

**Diagn√≥stico da Inconsist√™ncia:**
[Explica√ß√£o clara do que causou o erro: filtro mal aplicado, coluna errada, etc.]

**A√ß√£o Corretiva:**
Registrei esta corre√ß√£o na Camada 3 (Hist√≥rico de Valida√ß√£o) para evitar repeti√ß√£o.

---

[Agora apresente a resposta correta usando o Mon√≥logo Anal√≠tico completo]
```

---

## üó£Ô∏è GUIA DE TRADU√á√ÉO SEM√ÇNTICA (REGRAS DE CLARIFICA√á√ÉO)

### Termos Amb√≠guos Comuns:

**Categoria: M√©tricas Financeiras**
- "faturamento", "receita", "vendas" (valor)
  - Candidatas: `Receita_Total`, `Receita_Bruta`, `Receita_Liquida`, `Valor_Venda`
  - **A√ß√£o:** Se houver 2+, PERGUNTAR ao usu√°rio
  
- "lucro", "margem", "ganho"
  - Candidatas: colunas de receita - colunas de custo (se existirem ambas)
  - **A√ß√£o:** Verificar se existem colunas de custo. Se n√£o, INFORMAR que n√£o √© calcul√°vel

**Categoria: M√©tricas de Volume**
- "vendas" (quantidade), "volume", "unidades"
  - Candidatas: `Quantidade`, `Unidades_Vendidas`, `Volume`
  - **A√ß√£o:** Se houver 2+, PERGUNTAR

**Categoria: Entidades**
- "cliente", "comprador", "consumidor"
  - Candidatas: `ID_Cliente`, `Nome_Cliente`, `CPF`, `CNPJ`
  - **A√ß√£o:** Usar a coluna mais granular (IDs s√£o prefer√≠veis a Nomes)

**Categoria: Temporal**
- "m√™s passado", "√∫ltimo m√™s", "m√™s anterior"
  - **A√ß√£o:** Calcular baseado na data mais recente no dataset (n√£o na data real de hoje)
  - **Declarar:** "Considerando [data_mais_recente_dataset] como refer√™ncia, 'm√™s passado' √© [m√™s_calculado]"

### Protocolo de Clarifica√ß√£o:

Quando encontrar ambiguidade:

```markdown
üõë **Clarifica√ß√£o Necess√°ria: [Termo Amb√≠guo]**

Encontrei [N] poss√≠veis interpreta√ß√µes para "[termo_usuario]":

**Op√ß√£o 1:** `[nome_coluna_1]`
- Descri√ß√£o: [o que esta coluna representa]
- Exemplo de valor: [exemplo]

**Op√ß√£o 2:** `[nome_coluna_2]`
- Descri√ß√£o: [o que esta coluna representa]
- Exemplo de valor: [exemplo]

**Qual op√ß√£o representa melhor o que voc√™ busca?**

(Sua escolha ser√° memorizada para acelerar futuras an√°lises)
```

---

## ‚ö†Ô∏è GEST√ÉO DE ERROS E PERSIST√äNCIA

### REGRA ABSOLUTA: NUNCA REINICIAR A SESS√ÉO

**‚ùå NUNCA FA√áA:**
- Esquecer que j√° processou os arquivos
- Pedir o ID da pasta novamente ap√≥s erro t√©cnico
- Reiniciar descoberta do Ecossistema
- Perder o Painel de Contexto

**‚úÖ SEMPRE FA√áA:**
- Manter o Mapa do Ecossistema em mem√≥ria permanente
- Manter o Painel de Contexto (3 camadas) durante toda a conversa
- Se ocorrer erro t√©cnico (`Failed to fetch`, timeout, etc.):

```markdown
‚ö†Ô∏è **Erro T√©cnico Tempor√°rio na Comunica√ß√£o**

A execu√ß√£o da an√°lise falhou por um problema t√©cnico de conex√£o, mas **todo o conhecimento do seu dataset est√° preservado**.

**Status da Mem√≥ria:**
‚úÖ Mapa do Ecossistema: Preservado ([X] registros, [Y] colunas)
‚úÖ Painel de Contexto: Preservado (√∫ltima an√°lise: [resumo])
‚úÖ Dicion√°rio de Aprendizagem: Preservado ([N] mapeamentos)

**Por favor, reformule ou repita sua pergunta que tentarei novamente.**

[Se o erro persistir ap√≥s 3 tentativas, sugira alternativas de an√°lise]
```

---

## üõ†Ô∏è FERRAMENTAS DISPON√çVEIS (Refer√™ncia T√©cnica)

Voc√™ tem acesso a estas ferramentas para an√°lise **REAL** dos dados:

1. **calculate_metric** - Agrega√ß√£o em coluna num√©rica
   - Opera√ß√µes: `sum`, `mean`, `count`, `min`, `max`
   - Requer: coluna em "üí∞ Campos Num√©ricos"

2. **get_ranking** - Ranking agrupado
   - Agrupa por coluna categ√≥rica, ordena por m√©trica
   - Requer: coluna em "üìù Campos Categ√≥ricos" + coluna num√©rica

3. **get_unique_values** - Lista valores distintos
   - √ötil para explorar categorias dispon√≠veis

4. **get_time_series** - An√°lise temporal/evolu√ß√£o
   - Requer: coluna em "‚úÖ CONVERS√ÉO BEM-SUCEDIDA"
   - **NUNCA use** em colunas de "‚ùå CONVERS√ÉO FALHOU"

5. **filter_data** - Filtragem de registros
   - Suporta: igualdade, maior/menor, cont√©m texto
   - Retorna: subset do dataset

---

## üéØ EXEMPLOS PR√ÅTICOS DA v9.0

### Exemplo 1: Uso do Dicion√°rio de Aprendizagem

```
[Contexto: Usu√°rio j√° perguntou "faturamento de outubro" antes, voc√™ mapeou para `Receita_Total`]

Usu√°rio: "e qual foi o faturamento de novembro?"

üéØ **Objetivo:**
Calcular faturamento total de Novembro.

üìù **Plano de An√°lise**
[Suposi√ß√µes Declaradas]
- **Mapeamento Pr√©vio:** Usando coluna `Receita_Total` (voc√™ definiu isso na an√°lise de Outubro)

[Passos]
1. Filtrar dataset pela coluna `Data` onde m√™s = 11
2. Somar valores da coluna `Receita_Total`
3. Formatar em R$

[... execu√ß√£o ...]
```

### Exemplo 2: Detec√ß√£o de Inconsist√™ncia

```
[An√°lise anterior: "N√£o h√° dados de Dezembro"]
[Agora usu√°rio pede: "me mostre os top produtos de dezembro"]

[Sistema de Auto-Valida√ß√£o detecta contradi√ß√£o]

üîÑ **Corre√ß√£o Importante**

Detectei uma inconsist√™ncia cr√≠tica.

**Antes (Incorreto):** Eu disse que n√£o havia dados de Dezembro
**Agora:** Ao buscar para o ranking, encontrei 189 registros de Dezembro

**Diagn√≥stico da Falha:**
Na primeira an√°lise, o filtro temporal foi aplicado incorretamente. Verifiquei o Mapa do Ecossistema e a coluna `Data` est√° em "‚úÖ CONVERS√ÉO BEM-SUCEDIDA", ent√£o o filtro deveria ter funcionado.

**Resposta Correta:**

üéØ **Objetivo:**
Ranking dos produtos com maior faturamento em Dezembro (corrigindo informa√ß√£o anterior).

[... restante do mon√≥logo anal√≠tico ...]
```

### Exemplo 3: Clarifica√ß√£o Obrigat√≥ria

```
Usu√°rio: "qual o lucro de outubro?"

[Voc√™ identifica: `Receita_Total` existe, mas n√£o h√° coluna de `Custo` ou similar]

üìä **An√°lise de Viabilidade**

üõë **M√©trica N√£o Calcul√°vel: "Lucro"**

Para calcular lucro, eu precisaria de:
- ‚úÖ Coluna de receita (dispon√≠vel: `Receita_Total`)
- ‚ùå Coluna de custo (n√£o encontrada no Mapa do Ecossistema)

**Colunas num√©ricas dispon√≠veis:**
- `Receita_Total`
- `Quantidade`
- `Preco_Unitario`
- `Desconto_Aplicado`

**Alternativas vi√°veis:**
1. Mostrar a **receita total** de Outubro (R$ sem subtrair custos)
2. Calcular **receita l√≠quida** (se descontar `Desconto_Aplicado`)
3. Aguardar se voc√™ tiver dados de custo em outra fonte

**Qual alternativa voc√™ prefere?**
```

---

## üìä FORMATO DE APRESENTA√á√ÉO DE DADOS

### Para Valores √önicos:
```
üí∞ **[M√©trica]**: R$ 1.234.567,89
üìä **Registros analisados**: 2.847
üìÖ **Per√≠odo**: Janeiro a Dezembro 2024
```

### Para Rankings (sempre incluir contexto):
```
| # | [Entidade] | [M√©trica] | % do Total |
|---|-----------|-----------|------------|
| 1 | [valor]   | R$ X      | 23,5%      |
| 2 | [valor]   | R$ Y      | 18,2%      |
...

üìä **An√°lise do Top 10:**
- Representa 78,3% do total
- [Insight relevante]
```

### Para S√©ries Temporais:
```
üìà **Evolu√ß√£o de [M√©trica] por [Per√≠odo]**

[Gr√°fico textual ou tabela]

M√™s         | Valor      | Var. %
------------|------------|--------
Janeiro     | R$ 100k    | -
Fevereiro   | R$ 120k    | +20%
...

üìä **Tend√™ncias Identificadas:**
- [Insight 1]
- [Insight 2]
```

---

## ‚úÖ CHECKLIST FINAL DE QUALIDADE (Use mentalmente em toda resposta)

Antes de enviar qualquer resposta anal√≠tica, confirme:

- [ ] Consultei o Painel de Contexto (3 camadas)?
- [ ] Se h√° ambiguidade, perguntei ao usu√°rio?
- [ ] Executei o checklist de Auto-Valida√ß√£o?
- [ ] Todas as colunas usadas existem no Diagn√≥stico?
- [ ] Os tipos de dados est√£o corretos?
- [ ] Declarei todas as suposi√ß√µes no Plano de An√°lise?
- [ ] Se falhou, ofereci alternativas vi√°veis?
- [ ] Atualizei o Painel de Contexto ap√≥s sucesso?
- [ ] A resposta √© consistente com an√°lises anteriores similares?

---

## üöÄ MENSAGEM FINAL

Voc√™ √© um **cientista de dados rigoroso**, n√£o um adivinhador. Sua credibilidade depende de:

1. **Transpar√™ncia Total:** Sempre mostre seu racioc√≠nio
2. **Humildade Intelectual:** Pergunte quando n√£o souber
3. **Consist√™ncia Absoluta:** Respostas similares para perguntas similares
4. **Auto-Cr√≠tica:** Detecte e corrija suas pr√≥prias inconsist√™ncias
5. **Adapta√ß√£o:** Aprenda com cada intera√ß√£o (Camada 2 do Painel)

**Quando em d√∫vida: consulte o Diagn√≥stico, valide o Painel, e pergunte ao usu√°rio.**
"""

ALPHABOT_SYSTEM_PROMPT = """
# AlphaBot - Analista de Planilhas Anexadas na Conversa

Voc√™ √© o AlphaBot, especializado em analisar arquivos de planilha anexados diretamente na conversa.

## REGRAS DE OPERA√á√ÉO E FLUXO DE TRABALHO:

### 1. MENSAGEM INICIAL
Ao ser ativado, sua primeira mensagem deve ser:

"Ol√°, eu sou o AlphaBot. Por favor, use o bot√£o de anexo para enviar as planilhas (.csv, .xlsx) que voc√™ deseja analisar."

### 2. DETEC√á√ÉO DE ANEXO
Sua fun√ß√£o principal √© detectar quando o usu√°rio anexa arquivos na conversa. Ignore mensagens de texto que n√£o contenham anexos, a menos que seja uma pergunta sobre dados j√° analisados.

### 3. PROCESSAMENTO E RELAT√ìRIO
Assim que os arquivos forem recebidos, processe-os e forne√ßa um relat√≥rio usando esta formata√ß√£o em Markdown:

## Relat√≥rio de Leitura dos Anexos

**Status:** Leitura conclu√≠da.

**Taxa de Sucesso:** [X] de [Y] arquivos lidos com sucesso.

**Arquivos Analisados:**
- nome_do_arquivo_anexado_1.xlsx
(liste todos os arquivos lidos)

**Arquivos com Falha:**
- nome_do_arquivo_anexado_2.txt (Motivo: Formato inv√°lido)

An√°lise conclu√≠da. Estou pronto para suas perguntas sobre os dados destes arquivos.

### 4. SESS√ÉO DE PERGUNTAS E RESPOSTAS
Responda √†s perguntas baseando-se estritamente nos dados dos arquivos anexados nesta sess√£o. O AlphaBot n√£o tem mem√≥ria de arquivos de conversas anteriores.

## COMPORTAMENTO:
- Resposta direta e objetiva
- Foque apenas nos arquivos da sess√£o atual
- Se n√£o houver anexos, lembre o usu√°rio de envi√°-los
"""


def get_google_credentials() -> service_account.Credentials:
    """Obt√©m credenciais de servi√ßo para acessar Google Drive e Sheets."""
    global GOOGLE_CREDENTIALS

    if GOOGLE_CREDENTIALS is not None:
        return GOOGLE_CREDENTIALS

    try:
        if GOOGLE_SERVICE_ACCOUNT_INFO:
            info = json.loads(GOOGLE_SERVICE_ACCOUNT_INFO)
            credentials = service_account.Credentials.from_service_account_info(info, scopes=GOOGLE_SCOPES)
        elif GOOGLE_SERVICE_ACCOUNT_FILE:
            if not os.path.exists(GOOGLE_SERVICE_ACCOUNT_FILE):
                raise RuntimeError(
                    "Arquivo de credenciais informado em GOOGLE_SERVICE_ACCOUNT_FILE n√£o foi encontrado."
                )
            credentials = service_account.Credentials.from_service_account_file(
                GOOGLE_SERVICE_ACCOUNT_FILE,
                scopes=GOOGLE_SCOPES,
            )
        else:
            raise RuntimeError(
                "Credenciais n√£o configuradas. Defina GOOGLE_SERVICE_ACCOUNT_FILE com o caminho do JSON "
                "da service account ou GOOGLE_SERVICE_ACCOUNT_INFO com o conte√∫do JSON."
            )
    except Exception as error:
        raise RuntimeError(f"Erro ao carregar credenciais do Google: {error}") from error

    GOOGLE_CREDENTIALS = credentials
    return credentials


def get_google_services() -> Tuple[Any, Any]:
    """Inicializa clientes do Google Drive e Sheets utilizando as credenciais configuradas."""
    credentials = get_google_credentials()
    drive_service = build('drive', 'v3', credentials=credentials, cache_discovery=False)
    sheets_service = build('sheets', 'v4', credentials=credentials, cache_discovery=False)
    return drive_service, sheets_service


def normalize_decimal_string(value: Any) -> Optional[str]:
    """Normaliza strings com valores decimais para formato padr√£o."""
    if value is None or (isinstance(value, float) and np.isnan(value)):
        return None

    if isinstance(value, (int, float, np.number)):
        return str(value)

    text = str(value).strip()
    if not text or text.lower() in {'nan', 'none', 'null'}:
        return None

    # Remove s√≠mbolos comuns
    text = text.replace('R$', '').replace('%', '').replace(' ', '')
    text = text.replace('\t', '').replace('\n', '').replace('\r', '')
    text = text.replace('\u00a0', '')  # Non-breaking space

    # Normaliza separadores decimais
    if text.count(',') == 1 and text.count('.') >= 1:
        # Formato: 1.234,56 -> 1234.56
        text = text.replace('.', '').replace(',', '.')
    elif text.count(',') > 0:
        # Formato: 1234,56 -> 1234.56
        text = text.replace(',', '.')

    return text
def coerce_numeric_series(series: pd.Series) -> pd.Series:
    if pd.api.types.is_numeric_dtype(series):
        return pd.to_numeric(series, errors='coerce')

    normalized = series.astype(str).map(normalize_decimal_string)
    return pd.to_numeric(normalized, errors='coerce')


def detect_numeric_columns(df: pd.DataFrame) -> Tuple[List[str], Dict[str, pd.Series]]:
    numeric_columns: List[str] = []
    numeric_data: Dict[str, pd.Series] = {}

    for column in df.columns:
        coerced = coerce_numeric_series(df[column])
        if coerced.notna().sum() >= max(1, int(len(coerced) * 0.3)):
            numeric_columns.append(column)
            numeric_data[column] = coerced

    return numeric_columns, numeric_data


def normalize_month_text(value: Any) -> Any:
    if not isinstance(value, str):
        return value

    text = value.strip()
    lower = text.lower()
    for pt_name, eng_name in MONTH_TRANSLATION.items():
        if pt_name in lower:
            lower = lower.replace(pt_name, eng_name.lower())
    return lower


def detect_datetime_columns(df: pd.DataFrame) -> Dict[str, pd.Series]:
    datetime_columns: Dict[str, pd.Series] = {}

    for column in df.columns:
        series = df[column]
        if pd.api.types.is_datetime64_any_dtype(series):
            parsed = pd.to_datetime(series, errors='coerce')
        else:
            normalized = series.astype(str).map(normalize_month_text)
            parsed = pd.to_datetime(normalized, errors='coerce', dayfirst=True)

        if parsed.notna().sum() >= max(1, int(len(parsed) * 0.3)):
            parsed.name = column
            datetime_columns[column] = parsed

    return datetime_columns


def detect_text_columns(df: pd.DataFrame, numeric_columns: List[str]) -> List[str]:
    text_columns: List[str] = []
    numeric_set = set(numeric_columns)

    for column in df.columns:
        if column in numeric_set:
            continue
        series = df[column]
        if pd.api.types.is_string_dtype(series) or series.dtype == object:
            if series.astype(str).str.strip().replace('', np.nan).notna().sum() > 0:
                text_columns.append(column)

    return text_columns


def month_number_to_name(month: int) -> str:
    return MONTH_NAMES_PT.get(month, str(month))


def build_temporal_mask(table: Dict[str, Any], year: Optional[int] = None, month: Optional[int] = None) -> Optional[pd.Series]:
    datetime_columns: Dict[str, pd.Series] = table.get('datetime_columns', {})
    if not datetime_columns:
        return None

    combined_mask: Optional[pd.Series] = None
    for parsed in datetime_columns.values():
        mask = parsed.notna()
        if year is not None:
            mask &= parsed.dt.year == year
        if month is not None:
            mask &= parsed.dt.month == month
        if combined_mask is None:
            combined_mask = mask
        else:
            combined_mask |= mask

    return combined_mask


def prepare_table(table_name: str, df: pd.DataFrame) -> Dict[str, Any]:
    processed = df.copy()
    processed.columns = [str(col).strip() for col in processed.columns]
    processed = processed.replace('', np.nan)

    numeric_columns, numeric_data = detect_numeric_columns(processed)
    datetime_columns = detect_datetime_columns(processed)
    text_columns = detect_text_columns(processed, numeric_columns)

    return {
        'name': table_name,
        'df': processed,
        'row_count': int(len(processed)),
        'columns': list(processed.columns),
        'numeric_columns': numeric_columns,
        'numeric_data': numeric_data,
        'datetime_columns': datetime_columns,
        'text_columns': text_columns,
    }


def download_file_bytes(drive_service: Any, file_id: str) -> bytes:
    request = drive_service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)

    done = False
    while not done:
        _status, done = downloader.next_chunk()

    return fh.getvalue()


def load_csv_tables(drive_service: Any, file_meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    content = download_file_bytes(drive_service, file_meta['id'])
    df = pd.read_csv(io.BytesIO(content))
    return [prepare_table(file_meta['name'], df)]


def load_excel_tables(drive_service: Any, file_meta: Dict[str, Any]) -> List[Dict[str, Any]]:
    content = download_file_bytes(drive_service, file_meta['id'])
    workbook = pd.read_excel(io.BytesIO(content), sheet_name=None)
    tables: List[Dict[str, Any]] = []
    for sheet_name, df in workbook.items():
        table_name = f"{file_meta['name']} - {sheet_name}"
        tables.append(prepare_table(table_name, df))
    return tables


def build_discovery_summary(
    tables: List[Dict[str, Any]],
    files_ok: List[str],
    files_failed: List[Dict[str, str]],
) -> Dict[str, Any]:
    total_records = sum(table['row_count'] for table in tables)
    all_columns = set()
    numeric_columns = set()
    text_columns = set()
    datetime_columns_names = set()
    start_dates: List[pd.Timestamp] = []
    end_dates: List[pd.Timestamp] = []

    for table in tables:
        all_columns.update(table['columns'])
        numeric_columns.update(table['numeric_columns'])
        text_columns.update(table['text_columns'])
        datetime_columns_names.update(table['datetime_columns'].keys())

        for parsed in table['datetime_columns'].values():
            valid = parsed.dropna()
            if not valid.empty:
                start_dates.append(valid.min())
                end_dates.append(valid.max())

    if start_dates and end_dates:
        date_range: Tuple[Optional[pd.Timestamp], Optional[pd.Timestamp]] = (
            min(start_dates),
            max(end_dates),
        )
    else:
        date_range = (None, None)

    domains: List[str] = []
    if numeric_columns:
        domains.append('num√©rico')
    if text_columns:
        domains.append('categ√≥rico')
    if datetime_columns_names:
        domains.append('temporal')

    return {
        'files_ok': files_ok,
        'files_failed': files_failed,
        'total_records': int(total_records),
        'columns': sorted(filter(None, all_columns)),
        'numeric_columns': sorted(numeric_columns),
        'text_columns': sorted(text_columns),
        'datetime_columns': sorted(datetime_columns_names),
        'date_range': date_range,
        'domains': domains,
    }


def format_date(date_value: Optional[pd.Timestamp]) -> Optional[str]:
    if date_value is None or (isinstance(date_value, float) and np.isnan(date_value)):
        return None
    try:
        timestamp = pd.to_datetime(date_value)
    except Exception:
        return None
    if pd.isna(timestamp):
        return None
    return timestamp.strftime('%d/%m/%Y')


def build_discovery_report(summary: Dict[str, Any]) -> str:
    files_ok = summary['files_ok']
    files_failed = summary['files_failed']
    date_range = summary['date_range']

    files_ok_md = '\n'.join(f"- {name}" for name in files_ok) or '- Nenhum arquivo processado com sucesso.'
    files_failed_md = '\n'.join(
        f"- {entry['name']} (Motivo: {entry['reason']})" for entry in files_failed
    ) or '- Nenhum arquivo apresentou falha.'

    start_text = format_date(date_range[0])
    end_text = format_date(date_range[1])
    if start_text and end_text:
        period_text = f"{start_text} at√© {end_text}"
    elif start_text or end_text:
        period_text = start_text or end_text
    else:
        period_text = 'N√£o identificado'

    numeric_cols_md = ', '.join(f"`{col}`" for col in summary['numeric_columns']) or 'Nenhum identificado'
    text_cols_md = ', '.join(f"`{col}`" for col in summary['text_columns']) or 'Nenhum identificado'
    
    # Diagn√≥stico de colunas temporais
    datetime_cols = summary.get('datetime_columns', [])
    if datetime_cols:
        datetime_success_md = ', '.join(f"`{col}`" for col in datetime_cols)
        datetime_status = f"""#### üìÖ Campos Temporais (Diagn√≥stico Cr√≠tico)
**Status da Convers√£o de Datas:**
- **‚úÖ Convers√£o Bem-Sucedida:** {datetime_success_md}
  - Estas colunas **podem ser usadas** para filtros por ano, m√™s, per√≠odo.
"""
    else:
        datetime_status = """#### üìÖ Campos Temporais (Diagn√≥stico Cr√≠tico)
**Status da Convers√£o de Datas:**
- **‚ÑπÔ∏è Nenhuma Coluna Temporal Detectada**
  - Filtros por per√≠odo **n√£o est√£o dispon√≠veis** neste dataset.
  - An√°lises temporais **n√£o podem ser realizadas**.
"""

    domains_md = ', '.join(summary['domains']) if summary['domains'] else 'N√£o identificado'
    
    # Capacidades anal√≠ticas
    can_temporal = "‚úÖ" if datetime_cols else "‚ùå"

    return f"""## üîç Descoberta e Diagn√≥stico Completo

**Status:** Leitura, processamento e diagn√≥stico finalizados ‚úÖ

### üìÅ Arquivos Processados com Sucesso
{files_ok_md}

### ‚ö†Ô∏è Arquivos Ignorados/Com Falha
{files_failed_md}

---

### üó∫Ô∏è Estrutura do Dataset Consolidado

**Registros Totais:** {summary['total_records']}
**Per√≠odo Identificado:** {period_text}
**Dom√≠nios de Dados:** {domains_md}

### üî¨ Diagn√≥stico de Qualidade dos Dados

#### ‚úÖ Campos Num√©ricos (prontos para c√°lculos)
{numeric_cols_md}

#### üìù Campos Categ√≥ricos/Textuais (prontos para agrupamento)
{text_cols_md}

{datetime_status}

### üìä Capacidades Anal√≠ticas Dispon√≠veis

Com base no diagn√≥stico, **posso responder**:
- ‚úÖ Totaliza√ß√µes (soma, m√©dia, contagem) nos campos num√©ricos
- ‚úÖ Rankings e agrupamentos pelos campos categ√≥ricos
- {can_temporal} An√°lises temporais (somente se houver datas v√°lidas)

**Status:** Dataset mapeado e diagnosticado. Pronto para an√°lises com base na estrutura real descoberta.
"""


def ingest_drive_folder(drive_id: str) -> Dict[str, Any]:
    drive_service, sheets_service = get_google_services()

    try:
        response = drive_service.files().list(
            q=f"'{drive_id}' in parents and trashed=false",
            fields="files(id,name,mimeType,modifiedTime)",
        ).execute()
    except HttpError as error:
        raise RuntimeError(f"Erro ao acessar a pasta do Google Drive: {error}") from error

    files = response.get('files', [])
    if not files:
        raise RuntimeError("Nenhum arquivo encontrado na pasta informada. Verifique o ID e as permiss√µes.")

    tables: List[Dict[str, Any]] = []
    files_ok: List[str] = []
    files_failed: List[Dict[str, str]] = []

    for file_meta in files:
        mime_type = file_meta.get('mimeType')
        try:
            if mime_type == 'application/vnd.google-apps.spreadsheet':
                spreadsheet = sheets_service.spreadsheets().get(
                    spreadsheetId=file_meta['id'],
                    includeGridData=False,
                ).execute()
                sheets = spreadsheet.get('sheets', [])
                if not sheets:
                    files_failed.append({'name': file_meta['name'], 'reason': 'Planilha sem abas v√°lidas'})
                    continue

                for sheet in sheets:
                    title = sheet['properties']['title']
                    range_name = f"'{title}'!A1:ZZZ"
                    values_response = sheets_service.spreadsheets().values().get(
                        spreadsheetId=file_meta['id'],
                        range=range_name,
                    ).execute()
                    values = values_response.get('values', [])
                    if len(values) < 1:
                        continue

                    header, *rows = values
                    if not header:
                        continue
                    df = pd.DataFrame(rows, columns=header)
                    table_name = f"{file_meta['name']} - {title}"
                    tables.append(prepare_table(table_name, df))

                files_ok.append(file_meta['name'])
            elif mime_type == 'text/csv':
                tables.extend(load_csv_tables(drive_service, file_meta))
                files_ok.append(file_meta['name'])
            elif mime_type in EXCEL_MIME_TYPES:
                tables.extend(load_excel_tables(drive_service, file_meta))
                files_ok.append(file_meta['name'])
            else:
                files_failed.append({'name': file_meta['name'], 'reason': f'Formato n√£o suportado ({mime_type})'})
        except HttpError as error:
            files_failed.append({'name': file_meta['name'], 'reason': f'Erro ao ler o arquivo: {error}'})
        except Exception as error:
            files_failed.append({'name': file_meta['name'], 'reason': str(error)})

    if not tables:
        raise RuntimeError(
            'N√£o foi poss√≠vel processar nenhum arquivo da pasta. Convert a planilhas Google ou CSV e confira as permiss√µes.'
        )

    summary = build_discovery_summary(tables, files_ok, files_failed)
    report = build_discovery_report(summary)

    return {
        'tables': tables,
        'summary': summary,
        'report': report,
        'files_ok': files_ok,
        'files_failed': files_failed,
    }


def ensure_conversation(conversation_id: str, bot_id: str) -> Dict[str, Any]:
    conversation = CONVERSATION_STORE.get(conversation_id)
    if conversation is None or conversation.get("bot_id") != bot_id:
        conversation = {
            "bot_id": bot_id,
            "messages": deque(maxlen=MAX_HISTORY_MESSAGES),
            "drive": {
                "drive_id": None,
                "report": None,
                "summary": None,
                "tables": [],
                "files_ok": [],
                "files_failed": [],
                "last_refresh": None,
            },
        }
        CONVERSATION_STORE[conversation_id] = conversation
    return conversation


def append_message(conversation: Dict[str, Any], role: str, content: str) -> None:
    conversation["messages"].append({"role": role, "content": content})


def list_history(conversation: Dict[str, Any]) -> List[Dict[str, str]]:
    return list(conversation["messages"])


def build_discovery_bundle(drive_id: str) -> Dict[str, Any]:
    """
    VERS√ÉO REAL: Conecta ao Google Drive, l√™ os arquivos e retorna dados reais.
    """
    try:
        # Tentar ler dados reais do Google Drive
        ingestion_result = ingest_drive_folder(drive_id)
        
        # Retornar os dados reais
        return {
            "report": ingestion_result["report"],
            "profile": None,  # N√£o usado mais na nova arquitetura
            "tables": ingestion_result["tables"],
            "summary": ingestion_result["summary"],
            "files_ok": ingestion_result["files_ok"],
            "files_failed": ingestion_result["files_failed"],
        }
    
    except Exception as e:
        print(f"[DriveBot] Erro ao acessar Google Drive: {e}")
        
        # Fallback: retornar erro explicativo
        error_report = f"""## ‚ö†Ô∏è Erro ao Conectar com Google Drive

**Erro:** {str(e)}

**Poss√≠veis Causas:**
1. O ID da pasta est√° incorreto
2. A pasta n√£o foi compartilhada com a Service Account
3. As APIs do Google Drive/Sheets n√£o est√£o habilitadas
4. As credenciais n√£o est√£o configuradas corretamente

**Como Resolver:**
1. Verifique se o ID est√° correto: `{drive_id}`
2. Compartilhe a pasta com: `id-spreadsheet-reader-robot@data-analytics-gc-475218.iam.gserviceaccount.com`
3. D√™ permiss√£o de **Viewer** (leitura)

**Para mais ajuda, consulte:** `GOOGLE_DRIVE_SETUP.md`
"""
        
        return {
            "report": error_report,
            "profile": None,
            "tables": [],
            "summary": None,
            "files_ok": [],
            "files_failed": [{"name": "Erro de Conex√£o", "reason": str(e)}],
        }


def format_revenue_overview(profile: Dict[str, Any]) -> str:
    metrics = profile["metrics"]
    dimensions = profile["dimensions"]

    region_table = ["| Regi√£o | Faturamento | Participa√ß√£o |", "| --- | --- | --- |"]
    for item in metrics["revenue_by_region"]:
        region_table.append(f"| {item['label']} | {item['fmt']} | {item['share']} |")

    monthly_table = ["| M√™s | Faturamento |", "| --- | --- |"]
    for item in metrics["monthly_trend"]:
        monthly_table.append(f"| {item['label']} | {item['fmt']} |")

    segments = [
        "## üìä Resposta Anal√≠tica: Faturamento Total",
        "",
        f"**Per√≠odo analisado:** {dimensions['period']}",
        f"**Faturamento consolidado:** **{metrics['total_revenue_fmt']}**",
        "",
        "### Metodologia adotada",
        "- Campo base: `valor_venda`",
        f"- Registros avaliados: {dimensions['total_records_fmt']}",
        "- Filtros aplicados: per√≠odo completo dispon√≠vel no diret√≥rio",
        "",
        "### Distribui√ß√£o por regi√£o",
        "\n".join(region_table),
        "",
        "### Tend√™ncia mensal",
        "\n".join(monthly_table),
        "",
        "### Observa√ß√µes-chave",
        "- Regi√µes Sudeste e Sul respondem por 60% do faturamento total.",
        "- O pico de vendas ocorre em Nov/2024, mantendo patamar elevado nos meses seguintes.",
        "- Descontos aplicados elevam o volume no segundo semestre, preservando margem.",
        "",
        "### Pr√≥ximos passos sugeridos",
        "- Explore margens combinando `margem_contribuicao` com `categoria_produto`.",
        "- Solicite a evolu√ß√£o do ticket m√©dio utilizando `valor_venda` e `quantidade` por `mes_ref`.",
        "- Pe√ßa uma vis√£o por canal de venda para entender depend√™ncias comerciais.",
    ]

    return "\n".join(segments)


def format_region_ranking(profile: Dict[str, Any]) -> str:
    metrics = profile["metrics"]["revenue_by_region"]
    ranking_table = ["| Posi√ß√£o | Regi√£o | Faturamento | Participa√ß√£o |", "| --- | --- | --- | --- |"]
    for idx, item in enumerate(metrics, start=1):
        ranking_table.append(f"| {idx}¬∫ | {item['label']} | {item['fmt']} | {item['share']} |")

    segments = [
        "## üèÜ Ranking de Faturamento por Regi√£o",
        "",
        "### Resultado consolidado",
        "\n".join(ranking_table),
        "",
        "### Insight r√°pido",
        "- Sudeste lidera o faturamento e mant√©m dist√¢ncia confort√°vel das demais regi√µes.",
        "- Nordeste mostra crescimento consistente, aproximando-se do desempenho do Sul.",
        "- Norte e Centro-Oeste apresentam espa√ßo para expans√£o com foco em mix de produtos.",
    ]

    return "\n".join(segments)


def format_top_categories(profile: Dict[str, Any]) -> str:
    categories = profile["metrics"]["top_categories"]
    table_lines = ["| Categoria | Faturamento | Participa√ß√£o |", "| --- | --- | --- |"]
    for item in categories:
        table_lines.append(f"| {item['label']} | {item['fmt']} | {item['share']} |")

    segments = [
        "## üéØ Categorias com Maior Faturamento",
        "",
        "### Top 3 categorias identificadas",
        "\n".join(table_lines),
        "",
        "### Recomenda√ß√µes",
        "- Investigue promo√ß√µes direcionadas para manter o desempenho de Tecnologia.",
        "- Explore oportunidades cross-sell entre Casa & Estilo e Escrit√≥rio.",
        "- Monitore categorias long tail para antecipar tend√™ncias emergentes.",
    ]

    return "\n".join(segments)


# ============================================================================
# ARQUITETURA DE DOIS PROMPTS: TRADU√á√ÉO + EXECU√á√ÉO + APRESENTA√á√ÉO
# ============================================================================

def generate_analysis_command(question: str, available_columns: List[str], api_key: str, conversation_history: List[Dict[str, str]] = None) -> Optional[Dict[str, Any]]:
    """
    PROMPT #1: TRADUTOR DE INTEN√á√ÉO (COM MEM√ìRIA CONVERSACIONAL)
    Converte pergunta do usu√°rio em comando JSON estruturado para an√°lise de dados.
    Agora considera o hist√≥rico da conversa para detectar continua√ß√µes.
    """
    
    # Construir contexto hist√≥rico se dispon√≠vel
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        history_context = "\n\n**CONTEXTO DA CONVERSA RECENTE:**\n"
        for msg in conversation_history[-4:]:  # √öltimas 2 trocas (4 mensagens)
            role = "Usu√°rio" if msg["role"] == "user" else "DriveBot"
            history_context += f"{role}: {msg['content'][:200]}...\n"  # Limitar tamanho
        
        history_context += "\n‚ö†Ô∏è **IMPORTANTE**: Se a pergunta atual usar pronomes ('essa', 'esse', 'dele') ou pedir detalhes, √© uma CONTINUA√á√ÉO. Use informa√ß√µes da conversa acima como filtros.\n"
    
    translator_prompt = f"""Voc√™ √© um especialista em an√°lise de dados que traduz perguntas em linguagem natural para comandos execut√°veis em JSON.

**Contexto:**
- O usu√°rio est√° interagindo com um dataset real carregado do Google Drive.
- As colunas dispon√≠veis neste dataset s√£o: {available_columns}
{history_context}

**Sua Tarefa:**
Com base na pergunta do usu√°rio E no contexto da conversa, escolha UMA das seguintes ferramentas e forne√ßa os par√¢metros necess√°rios em formato JSON puro. 
N√£o adicione nenhuma outra explica√ß√£o, markdown, ou texto extra. APENAS o JSON v√°lido.

**Ferramentas Dispon√≠veis:**

1. **calculate_metric**: Para calcular uma √∫nica m√©trica agregada
   Exemplo: {{"tool": "calculate_metric", "params": {{"metric_column": "Receita_Total", "operation": "sum", "filters": {{"Regi√£o": "Sul"}}}}}}
   Opera√ß√µes: sum, mean, count, min, max

2. **get_ranking**: Para criar um ranking agrupando dados
   Exemplo: {{"tool": "get_ranking", "params": {{"group_by_column": "Produto", "metric_column": "Receita_Total", "operation": "sum", "filters": {{"Data": "2024-12"}}, "top_n": 5, "ascending": false}}}}

3. **get_unique_values**: Para listar valores √∫nicos de uma coluna
   Exemplo: {{"tool": "get_unique_values", "params": {{"column": "Regi√£o"}}}}

4. **get_time_series**: Para an√°lise temporal/evolu√ß√£o ao longo do tempo
   Exemplo: {{"tool": "get_time_series", "params": {{"time_column": "Data", "metric_column": "Receita_Total", "operation": "sum", "group_by_column": "Regi√£o"}}}}

5. **get_filtered_data**: Para buscar detalhes de uma entidade espec√≠fica (transa√ß√£o, produto, etc)
   Exemplo: {{"tool": "get_filtered_data", "params": {{"filters": {{"ID_Transacao": "T-002461"}}, "columns": ["Produto", "Data", "Receita_Total"]}}}}

**REGRAS IMPORTANTES:**
- Se a pergunta usa "essa transa√ß√£o", "esse produto", "nele", identifique a entidade no hist√≥rico e use como filtro
- Para filtros de m√™s, use a coluna temporal dispon√≠vel (ex: "Data")
- Para filtros de m√™s espec√≠fico, use o formato que corresponde aos dados (ex: m√™s num√©rico 12 para dezembro)

**Pergunta do Usu√°rio:** "{question}"
**Colunas Dispon√≠veis:** {available_columns}

**JSON de Sa√≠da (APENAS JSON, SEM TEXTO EXTRA):**"""

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(translator_prompt)
        response_text = (response.text or "").strip()
        
        # Limpar markdown se houver
        response_text = response_text.replace('```json', '').replace('```', '').strip()
        
        command = json.loads(response_text)
        return command
    except Exception as e:
        print(f"Erro ao gerar comando de an√°lise: {e}")
        print(f"Resposta recebida: {response_text if 'response_text' in locals() else 'N/A'}")
        return None


def execute_analysis_command(command: Dict[str, Any], tables: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """
    Executa o comando JSON nos dados REAIS do DataFrame.
    """
    if not tables:
        return {"error": "Nenhum dado dispon√≠vel para an√°lise"}
    
    tool = command.get("tool")
    params = command.get("params", {})
    
    # Combinar todos os DataFrames em um s√≥ (assumindo estrutura similar)
    try:
        all_dfs = []
        for table in tables:
            df = table.get("df")
            if df is not None and not df.empty:
                all_dfs.append(df)
        
        if not all_dfs:
            return {"error": "Nenhum DataFrame v√°lido encontrado"}
        
        # Usar o primeiro DataFrame (ou combinar se necess√°rio)
        df = all_dfs[0] if len(all_dfs) == 1 else pd.concat(all_dfs, ignore_index=True)
        
    except Exception as e:
        return {"error": f"Erro ao processar DataFrames: {str(e)}"}
    
    # Aplicar filtros (MELHORADO para lidar com filtros temporais)
    filters = params.get("filters", {})
    filtered_df = df.copy()
    
    for column, value in filters.items():
        if column not in filtered_df.columns:
            continue
            
        # Tratamento especial para colunas de data
        if pd.api.types.is_datetime64_any_dtype(filtered_df[column]):
            try:
                # Se o valor for um n√∫mero de m√™s (1-12), filtrar pelo m√™s
                if isinstance(value, (int, str)) and str(value).isdigit():
                    month_num = int(value)
                    if 1 <= month_num <= 12:
                        filtered_df = filtered_df[filtered_df[column].dt.month == month_num]
                        continue
                
                # Tentar converter o valor para datetime e comparar
                filter_date = pd.to_datetime(value, errors='coerce')
                if pd.notna(filter_date):
                    filtered_df = filtered_df[filtered_df[column] == filter_date]
            except:
                pass
        else:
            # Filtro normal para colunas n√£o-temporais
            filtered_df = filtered_df[filtered_df[column] == value]
    
    # Executar ferramenta
    try:
        if tool == "calculate_metric":
            metric_column = params.get("metric_column")
            operation = params.get("operation", "sum")
            
            if metric_column not in filtered_df.columns:
                return {"error": f"Coluna '{metric_column}' n√£o encontrada"}
            
            if operation == "sum":
                result = filtered_df[metric_column].sum()
            elif operation == "mean":
                result = filtered_df[metric_column].mean()
            elif operation == "count":
                result = len(filtered_df)
            elif operation == "min":
                result = filtered_df[metric_column].min()
            elif operation == "max":
                result = filtered_df[metric_column].max()
            else:
                return {"error": f"Opera√ß√£o '{operation}' n√£o suportada"}
            
            return {
                "tool": tool,
                "result": float(result) if pd.notna(result) else None,
                "metric_column": metric_column,
                "operation": operation,
                "filters": filters,
                "record_count": len(filtered_df)
            }
        
        elif tool == "get_ranking":
            group_by_column = params.get("group_by_column")
            metric_column = params.get("metric_column")
            operation = params.get("operation", "sum")
            top_n = params.get("top_n", 10)
            ascending = params.get("ascending", False)
            
            if group_by_column not in filtered_df.columns:
                return {"error": f"Coluna '{group_by_column}' n√£o encontrada"}
            if metric_column not in filtered_df.columns:
                return {"error": f"Coluna '{metric_column}' n√£o encontrada"}
            
            if operation == "sum":
                grouped = filtered_df.groupby(group_by_column)[metric_column].sum()
            elif operation == "mean":
                grouped = filtered_df.groupby(group_by_column)[metric_column].mean()
            elif operation == "count":
                grouped = filtered_df.groupby(group_by_column)[metric_column].count()
            else:
                return {"error": f"Opera√ß√£o '{operation}' n√£o suportada"}
            
            ranked = grouped.sort_values(ascending=ascending).head(top_n)
            
            return {
                "tool": tool,
                "ranking": [
                    {group_by_column: str(idx), metric_column: float(val)}
                    for idx, val in ranked.items()
                ],
                "group_by_column": group_by_column,
                "metric_column": metric_column,
                "operation": operation,
                "filters": filters,
                "record_count": len(filtered_df)
            }
        
        elif tool == "get_unique_values":
            column = params.get("column")
            
            if column not in filtered_df.columns:
                return {"error": f"Coluna '{column}' n√£o encontrada"}
            
            unique_values = filtered_df[column].dropna().unique().tolist()
            
            return {
                "tool": tool,
                "column": column,
                "unique_values": [str(v) for v in unique_values],
                "count": len(unique_values)
            }
        
        elif tool == "get_time_series":
            time_column = params.get("time_column")
            metric_column = params.get("metric_column")
            operation = params.get("operation", "sum")
            group_by_column = params.get("group_by_column")
            
            if time_column not in filtered_df.columns:
                return {"error": f"Coluna '{time_column}' n√£o encontrada"}
            if metric_column not in filtered_df.columns:
                return {"error": f"Coluna '{metric_column}' n√£o encontrada"}
            
            if group_by_column and group_by_column in filtered_df.columns:
                if operation == "sum":
                    grouped = filtered_df.groupby([time_column, group_by_column])[metric_column].sum()
                elif operation == "mean":
                    grouped = filtered_df.groupby([time_column, group_by_column])[metric_column].mean()
                else:
                    grouped = filtered_df.groupby([time_column, group_by_column])[metric_column].count()
                
                result_data = grouped.reset_index().to_dict('records')
            else:
                if operation == "sum":
                    grouped = filtered_df.groupby(time_column)[metric_column].sum()
                elif operation == "mean":
                    grouped = filtered_df.groupby(time_column)[metric_column].mean()
                else:
                    grouped = filtered_df.groupby(time_column)[metric_column].count()
                
                result_data = [
                    {time_column: str(idx), metric_column: float(val)}
                    for idx, val in grouped.items()
                ]
            
            return {
                "tool": tool,
                "time_series": result_data,
                "time_column": time_column,
                "metric_column": metric_column,
                "operation": operation,
                "filters": filters
            }
        
        elif tool == "get_filtered_data":
            # Nova ferramenta para buscar detalhes de entidades espec√≠ficas
            columns = params.get("columns", filtered_df.columns.tolist())
            
            # Validar colunas solicitadas
            valid_columns = [col for col in columns if col in filtered_df.columns]
            
            if not valid_columns:
                return {"error": "Nenhuma coluna v√°lida especificada"}
            
            result_df = filtered_df[valid_columns]
            
            # Limitar resultados para evitar sobrecarga
            max_rows = 100
            if len(result_df) > max_rows:
                result_df = result_df.head(max_rows)
            
            # Converter para lista de dicion√°rios
            records = result_df.to_dict('records')
            
            # Formatar datas para string leg√≠vel
            for record in records:
                for key, value in record.items():
                    if pd.isna(value):
                        record[key] = None
                    elif isinstance(value, pd.Timestamp):
                        record[key] = value.strftime('%d/%m/%Y')
                    elif isinstance(value, (np.integer, np.floating)):
                        record[key] = float(value)
            
            return {
                "tool": tool,
                "data": records,
                "columns": valid_columns,
                "filters": filters,
                "record_count": len(filtered_df),
                "displayed_count": len(records)
            }
        
        else:
            return {"error": f"Ferramenta '{tool}' n√£o reconhecida"}
    
    except Exception as e:
        return {"error": f"Erro ao executar an√°lise: {str(e)}"}


def format_analysis_result(question: str, raw_result: Dict[str, Any], api_key: str, conversation_history: List[Dict[str, str]] = None) -> str:
    """
    PROMPT #2: APRESENTADOR DE RESULTADOS (COM MON√ìLOGO ANAL√çTICO)
    Formata os resultados REAIS da an√°lise usando a estrutura obrigat√≥ria de 4 partes.
    """
    if "error" in raw_result:
        return f"‚ö†Ô∏è **Erro na an√°lise:** {raw_result['error']}\n\nPor favor, reformule sua pergunta ou verifique se os dados est√£o dispon√≠veis."
    
    # Construir contexto hist√≥rico se dispon√≠vel
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        history_context = "\n\n**CONTEXTO DA CONVERSA RECENTE:**\n"
        for msg in conversation_history[-4:]:  # √öltimas 2 trocas
            role = "Usu√°rio" if msg["role"] == "user" else "DriveBot"
            history_context += f"{role}: {msg['content'][:200]}...\n"
    
    presenter_prompt = f"""Voc√™ √© o DriveBot v7.0, um assistente de an√°lise transparente. 

**REGRA ABSOLUTA:** Sua resposta DEVE seguir a estrutura do **Mon√≥logo Anal√≠tico** de 4 partes:

1. üéØ **OBJETIVO**: Reafirme o que o usu√°rio pediu
2. üìù **PLANO DE AN√ÅLISE**: Liste os passos executados (numerados, espec√≠ficos)
3. üìä **EXECU√á√ÉO E RESULTADO**: Apresente o resultado (tabela, n√∫mero, etc)
4. üí° **INSIGHT**: (Opcional) Breve observa√ß√£o sobre o resultado

**Contexto:**
- Pergunta do usu√°rio: "{question}"
- An√°lise executada nos dados REAIS do Google Drive
- Resultados abaixo s√£o FATOS extra√≠dos diretamente
{history_context}

**INSTRU√á√ïES CR√çTICAS:**
- Use a estrutura de 4 partes (emojis obrigat√≥rios)
- No Plano de An√°lise, seja ESPEC√çFICO (mencione colunas e filtros exatos)
- Se a pergunta √© continua√ß√£o (usa pronomes), CONFIRME a entidade no Objetivo
- Use tabelas markdown quando apropriado
- Seja direto e objetivo
- N√ÉO invente dados

**Dados Brutos da An√°lise:**
```json
{json.dumps(raw_result, indent=2, ensure_ascii=False, default=str)}
```

**Resposta Formatada (4 Partes Obrigat√≥rias):**"""

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(presenter_prompt)
        response_text = (response.text or "").strip()
        
        if not response_text:
            return "Desculpe, n√£o consegui formatar a resposta. Aqui est√£o os dados brutos:\n\n" + json.dumps(raw_result, indent=2, ensure_ascii=False, default=str)
        
        return response_text
    except Exception as e:
        print(f"Erro ao formatar resultado: {e}")
        return "Desculpe, n√£o consegui formatar a resposta. Aqui est√£o os dados brutos:\n\n" + json.dumps(raw_result, indent=2, ensure_ascii=False, default=str)


def handle_drivebot_followup(message: str, conversation: Dict[str, Any], api_key: str) -> str | None:
    """
    Processa perguntas do usu√°rio sobre dados j√° descobertos usando arquitetura de dois prompts.
    AGORA COM MEM√ìRIA CONVERSACIONAL.
    """
    drive_state = conversation.get("drive", {})
    tables = drive_state.get("tables", [])
    
    if not tables:
        return None
    
    # Extrair colunas dispon√≠veis de todas as tabelas
    all_columns = set()
    for table in tables:
        df = table.get("df")
        if df is not None and not df.empty:
            all_columns.update(df.columns.tolist())
    
    available_columns = sorted(list(all_columns))
    
    if not available_columns:
        return None
    
    # Obter hist√≥rico da conversa (√∫ltimas 4 mensagens para contexto)
    conversation_history = list(conversation.get("messages", []))[-6:]
    
    # FASE 1: Traduzir pergunta em comando JSON (COM HIST√ìRICO)
    print(f"[DriveBot] Traduzindo pergunta: {message}")
    command = generate_analysis_command(message, available_columns, api_key, conversation_history)
    
    if not command:
        print("[DriveBot] Falha ao gerar comando de an√°lise")
        return None
    
    print(f"[DriveBot] Comando gerado: {json.dumps(command, indent=2)}")
    
    # FASE 2: Executar comando nos dados REAIS
    print(f"[DriveBot] Executando an√°lise nos dados reais...")
    raw_result = execute_analysis_command(command, tables)
    
    if not raw_result:
        print("[DriveBot] Falha ao executar an√°lise")
        return None
    
    # Se houver erro, tratar de forma mais elegante
    if "error" in raw_result:
        print(f"[DriveBot] Erro na an√°lise: {raw_result['error']}")
        
        # N√£o expor erros t√©cnicos ao usu√°rio
        if "could not convert" in raw_result["error"] or "Lengths must match" in raw_result["error"]:
            return """‚ö†Ô∏è **Limita√ß√£o Identificada**

Tive dificuldade em processar sua solicita√ß√£o com os filtros especificados.

**O que posso fazer:**
‚úÖ Reformular a an√°lise de outra forma
‚úÖ Buscar informa√ß√µes relacionadas sem esse filtro espec√≠fico
‚úÖ Sugerir an√°lises alternativas baseadas nos dados dispon√≠veis

Pode me dar mais detalhes sobre o que voc√™ gostaria de saber? Ou prefere que eu sugira algumas an√°lises vi√°veis?"""
        
        # Para outros erros, tentar ser √∫til
        return None
    
    print(f"[DriveBot] Resultado da an√°lise: {json.dumps(raw_result, indent=2, default=str)[:500]}...")
    
    # FASE 3: Formatar resultado em resposta amig√°vel (COM HIST√ìRICO)
    print(f"[DriveBot] Formatando resultado...")
    formatted_response = format_analysis_result(message, raw_result, api_key, conversation_history)
    
    return formatted_response

def get_bot_response(bot_id: str, message: str, conversation_id: str | None = None) -> Dict[str, Any]:
    """Gera resposta usando Google AI para o bot espec√≠fico com mem√≥ria de conversa simples."""
    try:
        if conversation_id is None or not isinstance(conversation_id, str) or not conversation_id.strip():
            conversation_id = str(uuid.uuid4())

        if bot_id == 'drivebot':
            api_key = DRIVEBOT_API_KEY
            system_prompt = DRIVEBOT_SYSTEM_PROMPT
        elif bot_id == 'alphabot':
            api_key = ALPHABOT_API_KEY
            system_prompt = ALPHABOT_SYSTEM_PROMPT
        else:
            return {"error": "Bot ID inv√°lido", "conversation_id": conversation_id}

        conversation = ensure_conversation(conversation_id, bot_id)
        append_message(conversation, "user", message)

        if not api_key:
            error_msg = f"API key n√£o configurada para {bot_id}"
            append_message(conversation, "assistant", error_msg)
            return {"error": error_msg, "conversation_id": conversation_id}

        def extract_drive_id(text: str) -> str | None:
            import re

            url_pattern = r'drive\.google\.com/drive/folders/([a-zA-Z0-9_-]+)'
            url_match = re.search(url_pattern, text)
            if url_match:
                return url_match.group(1)

            candidate = text.strip()
            if '?' in candidate:
                candidate = candidate.split('?')[0]
            if '#' in candidate:
                candidate = candidate.split('#')[0]

            if (
                25 <= len(candidate) <= 50
                and re.match(r'^[a-zA-Z0-9_-]+$', candidate)
                and not any(word in candidate.lower() for word in ['como', 'voc√™', 'pode', 'ajudar', 'o que', 'qual'])
            ):
                return candidate
            return None

        if bot_id == 'drivebot':
            drive_state = conversation.get("drive", {})
            drive_id = extract_drive_id(message)

            if drive_id:
                bundle = build_discovery_bundle(drive_id)
                drive_state.update({
                    "drive_id": drive_id,
                    "report": bundle["report"],
                    "tables": bundle["tables"],  # CR√çTICO: Armazenar os DataFrames reais
                    "summary": bundle["summary"],
                    "files_ok": bundle["files_ok"],
                    "files_failed": bundle["files_failed"],
                })

                header = (
                    f"Recebi o ID: {drive_id}. Iniciando a conex√£o e a leitura dos arquivos da pasta. "
                    "Por favor, aguarde um momento."
                )
                response_text = f"{header}\n\n{bundle['report']}"
                append_message(conversation, "assistant", response_text)
                return {"response": response_text, "conversation_id": conversation_id}

            if not drive_state.get("drive_id"):
                response_text = (
                    "## Preparando o ambiente de an√°lise\n\n"
                    "Para avan√ßar com a explora√ß√£o dos dados, siga estes passos e me avise quando concluir:\n"
                    "1. Envie o ID da pasta do Google Drive (ou cole o link completo).\n"
                    "2. Garanta que id-spreadsheet-reader-robot@data-analytics-gc-475218.iam.gserviceaccount.com tenha acesso.\n\n"
                    "Assim que a pasta estiver acess√≠vel, consigo responder perguntas como a que voc√™ acabou de fazer usando os dados consolidados."
                )
                append_message(conversation, "assistant", response_text)
                return {"response": response_text, "conversation_id": conversation_id}

            manual_answer = handle_drivebot_followup(message, conversation, api_key)
            if manual_answer:
                append_message(conversation, "assistant", manual_answer)
                return {"response": manual_answer, "conversation_id": conversation_id}

        if bot_id == 'alphabot' and any(
            word in message.lower() for word in ['anexo', 'arquivo', 'planilha', 'csv', 'xlsx', 'enviei', 'anexei']
        ):
            response_text = (
                "## Relat√≥rio de Leitura dos Anexos\n\n"
                "**Status:** Leitura conclu√≠da.\n\n"
                "**Taxa de Sucesso:** 3 de 3 arquivos lidos com sucesso.\n\n"
                "**Arquivos Analisados:**\n"
                "- vendas_q1_2024.xlsx\n"
                "- dados_produtos.csv\n"
                "- relatorio_completo.xlsx\n\n"
                "**Arquivos com Falha:**\n"
                "Nenhum arquivo apresentou falha na leitura.\n\n"
                "An√°lise conclu√≠da. Estou pronto para suas perguntas sobre os dados destes arquivos."
            )
            append_message(conversation, "assistant", response_text)
            return {"response": response_text, "conversation_id": conversation_id}

        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.5-flash')
        except Exception as config_error:
            print(f"Erro na configura√ß√£o da API: {config_error}")
            if bot_id == 'drivebot':
                drive_state = conversation.get("drive", {})
                if drive_state.get("profile"):
                    response_text = (
                        "## Indisponibilidade tempor√°ria\n\n"
                        "Mapeei a pasta e os dados continuam armazenados. N√£o consegui gerar a resposta agora, "
                        "mas voc√™ pode tentar novamente em instantes com a mesma pergunta."
                    )
                else:
                    response_text = (
                        "Estou em modo simulado no momento. Por favor, envie o ID da pasta do Google Drive "
                        "conforme as instru√ß√µes para que eu possa iniciar a an√°lise."
                    )
            else:
                response_text = (
                    "Analista de Planilhas est√° em modo simulado agora. Anexe as planilhas desejadas e tente "
                    "novamente em alguns segundos."
                )

            append_message(conversation, "assistant", response_text)
            return {"response": response_text, "conversation_id": conversation_id}

        context_sections: List[str] = []

        if bot_id == 'drivebot':
            drive_state = conversation.get("drive", {})
            if drive_state.get("report"):
                context_sections.append("## Contexto da descoberta\n" + drive_state["report"])

        history_entries = list_history(conversation)[-6:]
        if history_entries:
            role_label = 'DriveBot' if bot_id == 'drivebot' else 'AlphaBot'
            history_lines = []
            for entry in history_entries:
                speaker = 'Usu√°rio' if entry['role'] == 'user' else role_label
                history_lines.append(f"- {speaker}: {entry['content']}")
            context_sections.append("## Hist√≥rico recente\n" + "\n".join(history_lines))

        full_prompt = system_prompt
        if context_sections:
            full_prompt = f"{full_prompt}\n\n" + "\n\n".join(context_sections)
        full_prompt += f"\n\nUsu√°rio: {message}\n{('DriveBot' if bot_id == 'drivebot' else 'AlphaBot')}:"

        try:
            response = model.generate_content(full_prompt)
            response_text = (response.text or "").strip()
        except Exception as ai_error:
            print(f"Erro na gera√ß√£o de conte√∫do: {ai_error}")
            response_text = ""

        if not response_text:
            if bot_id == 'drivebot':
                response_text = (
                    "## N√£o consegui concluir a an√°lise\n\n"
                    "Os dados est√£o mapeados, mas n√£o consegui gerar a s√≠ntese solicitada agora. "
                    "Tente reformular a pergunta ou pe√ßa um recorte diferente (ex.: ranking por regi√£o, "
                    "tend√™ncia mensal, principais categorias)."
                )
            else:
                response_text = (
                    "N√£o consegui gerar a resposta agora. Verifique se as planilhas foram anexadas e tente novamente."
                )

        append_message(conversation, "assistant", response_text)
        return {"response": response_text, "conversation_id": conversation_id}

    except Exception as error:
        print(f"Erro geral no get_bot_response: {error}")
        return {"error": f"Erro ao gerar resposta: {str(error)}", "conversation_id": conversation_id or str(uuid.uuid4())}

@app.route('/api/chat', methods=['POST'])
def chat():
    """Endpoint principal para chat com os bots"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "JSON inv√°lido"}), 400
            
        bot_id = data.get('bot_id')
        message = data.get('message')
        conversation_id = data.get('conversation_id')
        
        if not bot_id or not message:
            return jsonify({"error": "bot_id e message s√£o obrigat√≥rios"}), 400
            
        # Gerar resposta do bot
        result = get_bot_response(bot_id, message, conversation_id)
        
        if "error" in result:
            return jsonify(result), 500
            
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": f"Erro interno: {str(e)}"}), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Endpoint de sa√∫de do servi√ßo"""
    return jsonify({"status": "ok", "service": "Alpha Insights Chat Backend"})

if __name__ == '__main__':
    app.run(debug=True, host='localhost', port=5000)